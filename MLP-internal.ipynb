{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from models.definitions.ANN import XANN, ANN\n",
    "\n",
    "from utils.utils import mass_fd, get_blobs\n",
    "from utils.data_loading import get_data_generator\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset constant dict\n",
    "num_samples = 1000\n",
    "feature_dim = 100\n",
    "transform = False\n",
    "split=0.95\n",
    "\n",
    "data_params = dict(feature_dim=feature_dim, split=split, num_samples=num_samples,\n",
    "                  cluster_std=[1.2, 0.8], plot=False, transform=transform)\n",
    "\n",
    "# Simple blobs with x% malicious nodes\n",
    "X, y = get_blobs(**data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader Parameters\n",
    "loader_params = {'batch_size': len(X),\n",
    "                  'shuffle': False,  # if you shuffle it will mix-up labels\n",
    "                  'sampler': 'balanced',\n",
    "                  'replacement': False,\n",
    "}\n",
    "\n",
    "train_split = 0.8\n",
    "\n",
    "# fetch the generators\n",
    "def fetch_generators(loader_params, data_params, train_split):\n",
    "    train_index = int(data_params['num_samples'] * train_split)\n",
    "    \n",
    "    return get_data_generator(X[:train_index], y[:train_index], loader_params), \\\n",
    "            get_data_generator(X[train_index:], y[train_index:], loader_params, validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_main_loop(model, train_gen, val_gen, optimizer, loss_fn, criterion, writer=None, verbose=False):\n",
    "\n",
    "    # main loop internal to the get_main_loop wrapper\n",
    "    def main_loop(phase, epoch=0):\n",
    "        \n",
    "        # set model phase and proper generator\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "            gen = train_gen\n",
    "        else:\n",
    "            model.eval()\n",
    "            gen = val_gen\n",
    "\n",
    "        # main training/validation loop\n",
    "        for x_batch, y_batch in gen: \n",
    "            output = model(x_batch, epoch, capture=True if phase == 'val' else False)\n",
    "            loss = loss_fn(output, y_batch)\n",
    "            accuracy = criterion(output, y_batch)\n",
    "            \n",
    "        # if training, compute and apply gradients\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # log performance\n",
    "        if writer is not None:\n",
    "            \n",
    "            if phase == 'train':\n",
    "                writer.add_scalar('train_loss', loss.item(), epoch)\n",
    "                writer.add_scalar('train_acc', accuracy, epoch)   \n",
    "            else:\n",
    "                writer.add_scalar('val_loss', loss.item(), epoch)\n",
    "                writer.add_scalar('val_acc', accuracy, epoch)       \n",
    "        \n",
    "        if verbose:\n",
    "            if not epoch % 10:\n",
    "                print(phase + f\" Epoch :{epoch:3d} | Loss: {loss.item():.4f} | Acc: {accuracy * 100:.2f}%\")\n",
    "                \n",
    "        return loss.item(), accuracy\n",
    "      \n",
    "    return main_loop\n",
    "\n",
    "def criterion(pred, y): \n",
    "    isPos = (y == 1.0)  # which examples are positive\n",
    "    return pred[isPos].ge(0.5).sum() / isPos.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration: 0\n",
      "Finished iteration: 1\n"
     ]
    }
   ],
   "source": [
    "# Used to test different hyperparameters on XANN architecture\n",
    "arch = [X.shape[1], 64, 32, 1]\n",
    "\n",
    "epochs = 10\n",
    "weight=False\n",
    "activations=False\n",
    "lr = 2e-3\n",
    "skip = 0\n",
    "norm=3.\n",
    "gyration=True\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "parameters = dict(lr=[1e-3],\n",
    "                  norm=[2.],\n",
    "                  gyration=[True],\n",
    "                  activations=[True, False])\n",
    "\n",
    "param_values = [v for v in parameters.values()]\n",
    "\n",
    "model_params = ['XANN1']\n",
    "\n",
    "for i, (lr, norm, gyration, activations) in enumerate(product(*param_values)):\n",
    "    \n",
    "    log_dir = \"logs/Grads_\" + f'{lr=}_{gyration=}_{activations=}_{weight=}_{norm=}_{gyration=}'\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # 1. Retreive the generators\n",
    "    train_gen, val_gen = fetch_generators(loader_params, data_params, train_split)\n",
    "\n",
    "    # 2. Init Model\n",
    "    model = XANN(arch, activations=activations, weight=weight, norm=norm, gyration=gyration, writer=writer) \n",
    "    #model = ANN(arch, writer=writer)\n",
    "\n",
    "    # 3. Prepare model related utilities\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # can add weighting factor\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    criterion = lambda pred, y: torch.sum(torch.round(torch.sigmoid(pred)) == y) / y.shape[0]\n",
    "\n",
    "    # 4. Fetch main loop\n",
    "    main_loop = get_main_loop(model, train_gen, val_gen,\n",
    "                              optimizer, loss_fn, criterion,\n",
    "                              writer=writer, verbose=False)\n",
    "\n",
    "    # 5. Start training\n",
    "    for epoch in range(epochs):\n",
    "        main_loop(phase='train', epoch=epoch)\n",
    "\n",
    "        # Run validation only 10 times\n",
    "        #if not epoch % (epochs // 10):\n",
    "        with torch.no_grad():\n",
    "            loss, acc = main_loop(phase='val', epoch=epoch)\n",
    "\n",
    "    writer.add_hparams({\"lr\": lr, \"norm\": norm, \"gyration\": gyration, 'activation': activations}, \n",
    "                       {\"val_loss\": loss, \"val_acc\": acc})\n",
    "    \n",
    "    print(f\"Finished iteration: {i}\") \n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to test different learning rates on simpe ANN architecture\n",
    "arch = [X.shape[1], 64, 32, 1]\n",
    "\n",
    "epochs = 500\n",
    "log = False\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "parameters = [1e-2]\n",
    "\n",
    "for i, lr in enumerate(parameters):\n",
    "    \n",
    "    if log:\n",
    "        log_dir = \"logs/\" + f'ANN_{lr=}'\n",
    "        writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # 1. Retreive the generators\n",
    "    train_gen, val_gen = fetch_generators(loader_params, data_params, train_split)\n",
    "\n",
    "    # 2. Init Model \n",
    "    model = ANN(arch, writer=writer)\n",
    "\n",
    "    # 3. Prepare model related utilities\n",
    "    loss_fn = nn.BCEWithLogitsLoss()  # can add weighting factor\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    #criterion = lambda pred, y: torch.sum(torch.round(torch.sigmoid(pred)) == y) / torch.sum(y == 1.0)\n",
    "\n",
    "    # 4. Fetch main loop\n",
    "    main_loop = get_main_loop(model, train_gen, val_gen,\n",
    "                              optimizer, loss_fn, criterion,\n",
    "                              writer=writer, verbose=True)\n",
    "\n",
    "    # 5. Start training\n",
    "    for epoch in range(epochs):\n",
    "        main_loop(phase='train', epoch=epoch)\n",
    "\n",
    "        # Run validation only 10 times\n",
    "        #if not epoch % (epochs // 10):\n",
    "        with torch.no_grad():\n",
    "            loss, acc = main_loop(phase='val', epoch=epoch)\n",
    "    \n",
    "    if log:\n",
    "        writer.add_hparams({\"lr\": lr, \"norm\": 0., \"gyration\": False, 'activation': False}, \n",
    "                           {\"val_loss\": loss, \"val_acc\": acc})\n",
    "\n",
    "    print(f\"Finished iteration: {i}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
