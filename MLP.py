
import torch
from torch import nn

from utils.utils import Mass_fd

class ANN(nn.Module):
    '''
    Simple ANN for binary classification using a reLu and sigmoid for final activation.

    '''
    def __init__(self, arch, writer=None):
        super().__init__()
        
        layers = []
        
        for d_in, d_out in zip(arch[:-1], arch[1:]):
            layers.append(nn.Linear(d_in, d_out))

        self.model = nn.Sequential(*layers)  # wrapper for the ANN layers
        self.reLu = nn.ReLU()
        self.writer = writer
        
    def forward(self, x, epoch, capture=False):
        
        for i, layer in enumerate(self.model[:-1]):
            z = layer(x)
            x = self.reLu(z)
            
            # used to captire net inputs and activations
            if capture:
                self.writer.add_histogram(f'z_{i}', z, epoch)
                self.writer.add_histogram(f'a_{i}', x, epoch)
            
        # final linear layer, no activation (using BCELoss)
        z = self.model[-1](x)
        
        return z.squeeze()
    


class XANN(nn.Module):
    '''
    Complexity based ANN - XANN - with a single hidden layer and a FD measure at the end.
    
    '''
    def __init__(self, arch, k=2, skip=0, gyration=True, activations=False, weight=False, norm=2., writer=None):
        super().__init__()
        
        layers = []
        
        for d_in, d_out in zip(arch[:-1], arch[1:]):
            layers.append(nn.Linear(d_in, d_out, bias=False))  # manually removed biases
            layers.append(Mass_fd(d_out, k=k, skip=skip, gyration=gyration, weight=weight, norm=norm))

        self.model = nn.Sequential(*layers)  # wrapper for the ANN layers
        
        # initialize activation functions
        self.reLu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        
        # whether to use FD post activations
        self.activations = activations
        
        self.writer = writer

        
    def forward(self, x, epoch, capture=False):

        # loop through hidden layers
        for i, layer in enumerate(self.model[:-1]):        
            
            # check if layer is fd_layer
            if isinstance(layer, Mass_fd):
                
                z = self.reLu(x)
                
                # if pre-activations; compute first then add to activation
                if not self.activations: 
                    fd = layer(x) 
                # if post-activations; activations first then add to activation
                else:
                    fd = layer(z)

                x = z + fd  
                
                # capture intermediate net inputs/activations
                if capture:
                    self.writer.add_histogram(f"a_{i}", z, epoch)
                    self.writer.add_scalar(f"fd_{i}", fd, epoch)
                    
            # must be linear layer
            else:
                x = layer(x)       
          
        # final activation before last layer
        z = self.reLu(x)                        
                                           
        # final layer of fd
        if not self.activations: 
            fd = self.model[-1](x)    
        # if post-activations; activations first then add to activation
        else:
            fd = self.model[-1](z)  
                                           
        output = z + fd

        # capture intermediate net inputs/activations
        if capture:
            self.writer.add_histogram(f"a_last", z, epoch)
            self.writer.add_scalar(f"fd_last", fd, epoch)
 
        return output.squeeze()
