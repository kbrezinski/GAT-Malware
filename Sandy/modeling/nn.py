import tensorflow as tf
import pandas as pd
import numpy as np

from tensorflow import feature_column
from sklearn.model_selection import train_test_split

class NNmodel(tf.keras.Model):


    def __init__(self, df, *args, **kwargs):
        super().__init__()

        ## Added this to catch those rougue NaNs
        self.df = df #df.replace(pd.NA, 0)
        self.layer_len = len(args[0])
        self.__dict__.update(kwargs)

        ## Create the sequential NN model
        self.create_features()
        self.create_model(args[0])


    def create_model(self, args):

        self._define_activations()
        self.model = tf.keras.Sequential()
        (self.model).add(tf.keras.layers.DenseFeatures(self.feature_columns, trainable=False))

        for neurons, activation in zip(args, self.activations):

            (self.model).add(tf.keras.layers.Dense(neurons, activation=activation))

            if self.dropout is not None:
                (self.model).add(tf.keras.layers.Dropout(self.dropout))

        (self.model).add(tf.keras.layers.Dense(1, activation='sigmoid'))


    def create_features(self, min_nan=0.1):

        feature_columns = []
        fc = tf.feature_column

        self.df = (self.df).loc[:, ((self.df).isnull().sum(axis=0) <= 0.1 * len((self.df).index))]

        feature_index = (self.df).columns.get_loc("targetIDX")
        self.df = (self.df).iloc[:, feature_index:]
        feature_range = (self.df).columns[feature_index + 1:]

        for header in feature_range:

            isNA = self.df[header].isna()

            if (self.df).dtypes[header] == float or (self.df).dtypes[header] == int:
                self.df[header][isNA] = 0
                feature_columns.append(fc.numeric_column(header, dtype=tf.float32))

            else:
                self.df[header][isNA] = "<UNK>"
                vocab = self.df[header][~isNA].unique()
                feature_columns.append(self._one_hot_cat_column(header, vocab, fc))

        self.feature_columns = feature_columns

    def compile_v2(self):
        ## Perform similar functionality than model.compile
        pass

    def plot_sum():
        pass


    def create_datasets(self, testsplit=0.2, validation=False, batch_size=32, num_epochs=None):

        train, test = train_test_split(self.df, test_size=testsplit)

        if validation:
            train, val = train_test_split(train, test_size=testsplit)
            self.val_ds = self._df_to_dataset(val, shuffle=False, num_epochs=1)

        self.train_ds = self._df_to_dataset(train, batch_size=batch_size)
        self.test_ds = self._df_to_dataset(test, shuffle=False, num_epochs=1)


        if self.VERBOSE:
            print(f"\nLength of train: {len(train)}")
            print(f"\nLength of test: {len(test)}")

            if validation:
                print(f"\nLength of validation: {len(val)}")

            benign, mal = np.bincount(self.df['targetIDX'])
            total = mal + benign

            print(f'\nTotal: {total} \nPositive: {mal} ({100 * mal / total:.2f}% of total)\n')

    def determine_class_weight(self):

        benign, mal = np.bincount(self.df['targetIDX'])
        total = mal + benign

        return {0 : 2 * mal / total, 1 : 2 * benign / total}


    @staticmethod
    def _one_hot_cat_column(feature, vocab, fc):

        return fc.indicator_column(
            fc.categorical_column_with_vocabulary_list(feature, vocab))


    @staticmethod
    def _df_to_dataset(dataframe, shuffle=True, num_epochs=None, batch_size=32):
        
        ## Returning copy of a slice from dataframe error
        dataframe = dataframe.copy()
        labels = dataframe.pop('targetIDX')
        ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))
        if shuffle:
            ds = ds.shuffle(buffer_size=len(dataframe))
        ds = ds.repeat(num_epochs).batch(batch_size)

        return ds


    def _define_activations(self):

        act_len = len(self.activations)

        if act_len == 1:
            self.activations = [self.activations[0]] * self.layer_len

        elif act_len == 0:
            self.activations = ['relu'] * self.layer_len

        elif act_len != self.layer_len:
            raise ValueError(f"Activation length mismatch: {act_len} entries for {self.layer_len} layers")

        else:
            pass
