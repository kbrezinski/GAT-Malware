
import os
import torch
import networkx as nx
import pickle
import pandas as pd
import xml.etree.ElementTree as ET

from collections import defaultdict
import numpy as np
from torch_geometric.data import InMemoryDataset, Data
from sklearn.feature_extraction.text import TfidfVectorizer

from config.utils import (assign_labels, get_edge_list, get_module_df,
                                get_stack_df, get_feature_vectors)


class MalwareDataset(InMemoryDataset):
    def __init__(self, root, cfg, transform=None, pre_transform=None):

        self.root = root
        self.event_type = cfg['event_type']
        self.transform = transform
        self.ngram = cfg['ngram']
        self.feature_set = cfg['features']
        
        super().__init__(root, transform, pre_transform)
        
    @property
    def raw_file_names(self):
        XML_PATH = os.path.join(self.root, 'xml')
        # if these files do not exists in /raw, the xml parse is not triggered
        return [filename.rsplit('.', 1)[0] + self.event_type + '.csv' for filename in os.listdir(XML_PATH)]
        
    @property
    def processed_file_names(self):
        # if this file exists in /processed, process is not triggered
        return [f"{self.feature_set}.{self.ngram}.{self.event_type}.pt"] 
    
    def download(self):
        # process the raw xml files if csv files are not present
        self.process_xml(self.root)

    def len(self):
        # only a single list of graphs is stored
        return 1

    def process_xml(self, base_path):
        print("Processing .XML files...")

        # append raw path to base_dir
        XML_PATH = os.path.join(base_path, 'xml')

        # loop through .XML files
        for filename in os.listdir(XML_PATH):
            
            # loop through relevent event type xml files
            if (not filename.endswith(f"{self.event_type}.XML")) and (self.event_type != 'all'):
                continue
 
            # check if the csv file already exists
            file_key_name = filename.rsplit(".", 1)[0] + '.csv'
            if file_key_name in os.listdir(os.path.join(XML_PATH, os.pardir, 'raw')):
                continue

            # get the full path and root of xml file
            path = os.path.join(base_path, 'xml', filename)

            # get the module df; join with stack traces, then formatting
            process_df = get_module_df(path)\
                        .join(get_stack_df(path), how='left')\
                        .reset_index()\
                        .fillna(value={'stack':' '})
                
            # create save dir and save it
            save_dir = os.path.join(base_path, 'raw')
            os.makedirs(save_dir, exist_ok=True)
            
            # get the target name of the malware executable and save it
            target_name = filename.split(".")[0].lower()
            save_name = f'{target_name}.{self.event_type}.csv'

            # create final df
            process_df.to_csv(
                    os.path.join(save_dir, save_name), index=False)
            print(f"Finished saving {save_name} to {save_dir}")

    # main process 
    def process(self):

        # check if the feature set is hetero
        if self.feature_set == 'hetero':
            self.process_hetero()
            return
        
        # parent list for all the executable graphs
        exe_graphs = []
        module_list = []

        # loop through all the files and read csv
        for filename in os.listdir(self.raw_dir):
            
            # check if all is selected, or wrong event type
            if (not filename.endswith(f"{self.event_type}.csv")) and (self.event_type != 'all'):
                print("TESTING")
                continue
                
            print(f"Processing {filename}...")

            # read the csv file; running into encoding issues
            graph_nodes = pd.read_csv(self.raw_dir + '\\' + filename, encoding='unicode_escape', engine='python')

            # Assign labels and return array
            target_list = assign_labels(graph_nodes, filename)
            
            # Get the processed feature list
            module_list.append(graph_nodes[self.feature_set])
            # Get the edge list
            edge_list = get_edge_list(graph_nodes)

            # Create the data object
            data = Data(
                y = target_list,
                edge_index = truncate_edges(edge_list, max_nodes=len(target_list)),
            )
            # append to master list
            exe_graphs.append((filename, data))

        # get the tfidf vectors for the modules
        tfidf_modules, vectorizer = get_feature_vectors(module_list, ngram=self.ngram, min_df=1)
        
        for graph, module in zip(exe_graphs, tfidf_modules):
            graph[1].x = module 

        # collate the list and save the data
        torch.save([exe[1] for exe in exe_graphs], self.processed_paths[0])

        # save the vectorizer
        save_name = f"{self.feature_set}.{self.ngram}.{self.event_type}.pkl"
        pickle.dump(vectorizer, open(f"models/binaries/{save_name}", "rwb"))


    # fetch the data from the .pt file
    def get(self, idx):
        return torch.load(self.processed_paths[0])


def truncate_edges(edge_index, max_nodes):
    # truncate edges to max_nodes: [2, E]
    edge_index = edge_index[:, edge_index[0] < max_nodes]
    edge_index = edge_index[:, edge_index[1] < max_nodes]
    return edge_index


# carries out tfidf vectorization on the modules per sample
def get_feature_vectors(module_list, ngram=(2, 2), min_df=1):
    
    new_list = []
    module_shapes = []
    # fill nans with space and collect sizes
    for module in module_list:
        module_shapes.append(module.shape[0])
        new_list.append(module.fillna('', inplace=False).tolist())
    #create flattened list
    flattened_list = [item for sublist in new_list for item in sublist]

    # create the vectorizer and fit
    vectorizer = TfidfVectorizer(ngram_range=ngram,
                                 lowercase=True,
                                 min_df=min_df)
    vectorizer = vectorizer.fit(flattened_list)
    vectorized_modules = vectorizer.transform(flattened_list).toarray()

    # some processing to make sure the vectors are the correct shape per sample
    vectorized_tfidf = [vectorized_modules[:module_shapes[0]]]
    for start, end in zip(module_shapes[:-1], module_shapes[1:]):
        vectorized_tfidf.append(vectorized_modules[start:start + end])

    # return the vectorized modules and the vectorizer
    vectorized_modules = [torch.tensor(exe, dtype=torch.float) for exe in vectorized_tfidf]

    return (vectorized_modules, vectorizer)


def get_stack_df(path) -> pd.DataFrame:
    """
    Placeholder
    """
    stack_dict = defaultdict(list)
    root = ET.parse(path).getroot()
    # loop through the events, then the tags
    for events in root[1]:
        for event in events:
            
            # get the PID
            if event.tag == 'PID':
                if event.tag not in stack_dict:
                    curr_pid = event.text

            # otherwise get the stack
            if event.tag == 'stack':
                # some nested xml tags
                for module in event:
                    for ele in module:
                        if ele.tag != 'location': continue
                        stack_dict[curr_pid].append(ele.text.split("+")[0][:-1])

    # string join the keys in stack_dict
    for key in stack_dict:
        stack_dict[key] = ' '.join(stack_dict[key])

    return pd.DataFrame(stack_dict, index=[0]).rename({0:'stack'}).T


def get_module_df(path) -> pd.DataFrame:
    """
    Placeholder
    """
    attr_dict = defaultdict(list)
    root = ET.parse(path).getroot()
    # loop through the events, then the tags
    for events in root[0]:
        for event in events:
            
            # get the module list if root[0] is selected
            if event.tag == 'modulelist':
                
                # build up the module list
                # the path tag is the 4th element in the module list
                empty_stack = [module[3].text.split('\\')[-1] for module in event]
                            
                # create the module feature
                attr_dict['module'].append(' '.join(empty_stack))

            else:
                attr_dict[event.tag].append(event.text)

    return pd.DataFrame(attr_dict).set_index('ProcessId')


def assign_labels(df, file_name):

    """
    Given a file name, this function assigns labels to the processes in the dataframe.
    The label is 1 if the process is malicious, 0 otherwise. The label is assigned
    recursively, starting from the processes that have the same name as the file name.
    """

    prev_len = -1
    target_name = file_name.split(".")[0]

    # get the malicious pids of the parent process
    df['ProcessName'] = df['ProcessName'].apply(lambda x: x.lower())
    unique_pids = df[df['ProcessName'] == target_name + '.exe']['ProcessId'].unique()

    if len(unique_pids) == 0:
        print(f"WARNING: Target name not found in {file_name}")
        prev_len = 0

    mal_pids = set(unique_pids)

    # stops when the length of pids stops growing
    while prev_len != len(mal_pids):
        new_rows = df[df['ParentProcessId'].isin(mal_pids)]
        new_pids = set(new_rows['ProcessId'].unique())

        # updates the lengths and the malicious pids
        prev_len = len(mal_pids)
        mal_pids.update(new_pids)

    df['label'] = df['ProcessId'].apply(lambda x: 1 if x in mal_pids else 0)
    
    return torch.tensor(df['label'].values, dtype=torch.long)


def get_edge_list(graph_nodes, add_self_edges=True, return_lookup=False):

    """
    Given a dataframe of process nodes, this function returns the edge list
    in the form of a numpy array. The edge list is a 2 x E array, where E is
    the number of edges. The first row contains the source nodes, and the second
    row contains the target nodes.
    """

    ## Create the adjacency list using nx
    G = nx.from_pandas_edgelist(graph_nodes, 'ProcessId', 'ParentProcessId')
    
    ## Create some empty lists
    trg, src = [], []
    pid_lookup = dict()
    
    ## Loop through the adjacency list
    for n, (source, nbrdict) in enumerate(G.adjacency()):

        if source not in pid_lookup:
            pid_lookup[source] = len(pid_lookup)

        for target in nbrdict.keys():
            if target not in pid_lookup:
                pid_lookup[target] = len(pid_lookup)

            src.append(pid_lookup[source])
            trg.append(pid_lookup[target])
            
    ## Add self-edges if set to true
    if add_self_edges:
        src.extend(list(range(len(pid_lookup))))
        trg.extend(list(range(len(pid_lookup))))
        
    ## Convert to numpy arrays [2, E] and cast to torch tensors
    edge_index = np.row_stack((src, trg))
    edge_index = torch.tensor(edge_index, dtype=torch.long)

    if return_lookup:
        return edge_index, pid_lookup

    return edge_index