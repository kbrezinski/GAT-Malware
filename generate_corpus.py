
import os
import pickle
import argparse
import numpy as np
import networkx as nx

from collections import defaultdict

from utils.config import *
from Sandy.graph import buildGraph
from Sandy.pp import importProc


def main(cfg):
    
    '''
    Fetches the node topologies from the malicious event types (.XML files), and creates a corpus of API calls. Can be
    preprocessed using ngrams or tf-idf at a later time. 
    
    Rather than separate the stack traces, this implementation just concatenates them together for each process.
    
    '''
    
    # Create empty master dict for all samples and attributes
    master_repo = defaultdict(list)
    
    # loop through .XML files
    for filename in os.listdir(DATA_DIR):
                
        if filename.endswith(f"{cfg['event_type']}.XML"):
            
            # Use sandy libraries to parse the stack traces and assign target
            graph_nodes = buildGraph.Network(os.path.join(SANDY_PATH, filename),
                                             import_type='mod', compress=False, track_process=False)
            graph_nodes.create_target(filename.split('.')[-3] + '.exe')
            graph_nodes = graph_nodes.object
            
            graph_events = buildGraph.Network(os.path.join(SANDY_PATH, filename),
                                              import_type='event', compress=False, track_process=False).object
            
            # returns as dict where {pid : S} where S is a list of stack traces for given pid
            pid_lookup = get_stack_traces(graph_events, graph_nodes)
                
            # returns the edge index, the node_list and target list
            edge_index, target_arr, node_arr, graph_top = get_edge_lists(graph_nodes, cfg['self_edges'])
            
            # append items to master dict
            # size = (n, {pid : S})
            master_repo['pid_lookup'].append(pid_lookup) 
            # size = (n, [y])
            master_repo['target_arr'].append(target_arr)
            # size = (n, [pid])
            master_repo['node_arr'].append(node_arr)
            # size = (n, [2, E])
            master_repo['edge_indices'].append(edge_index)
            
    # Generate corpus
    corpus = generate_corpus(master_repo['pid_lookup'], master_repo['node_arr'], cfg['ngram'])
                      
    # save intemediate data if flag is set
    if cfg['save']:
        pickle.dump(corpus,
                    open(os.path.join(SANDY_ATTR_PATH, f'corpus.{cfg["event_type"]}.pkl'), "wb" ) )  # list of strings
        pickle.dump(master_repo,
                    open(os.path.join(SANDY_ATTR_PATH, f'attr.{cfg["event_type"]}.pkl'), "wb" ) )  # list of strings
    
    
def pickle_save():
    pass
    
def generate_corpus(master_pid, master_node, ngram_range=(1)):
    
    # Generate the corpus from all the collection of pids and nodes
    corpus = []
    for pid_lookup, node_list in zip(master_pid, master_node): 
        for node in node_list:
            s = ''
            if node in pid_lookup:
                s += ' '.join(pid_lookup[node])
            corpus.append(s)
    
    return corpus
    
            
            
def get_edge_lists(graph_nodes, add_self_edges=True):
    
    ## Create the adjacency list and target attribute using nx
    G = nx.from_pandas_edgelist(graph_nodes, 'PID', 'PPID')
    target_map = {n:v for n, v in zip(graph_nodes['PID'], graph_nodes['targetIDX'])}
    nx.set_node_attributes(G, target_map, "target")
    
    ## Create some empty lists
    trg, src = [], []
    node_list = []
    target_list = []
    
    ## Loop through the adjacency list
    for n, nbrdict in G.adjacency():
        for target in nbrdict.keys():
            src.append(n)
            trg.append(target)
            
        ## Append the PID and the target
        node_list.append(n)
        target_list.append(G.nodes[n].get('target', 0))

    ## Add self-edges if set to true
    if add_self_edges:
        src.extend(node_list)
        trg.extend(node_list)
        
    ## Convert all to numpy arrays
    edge_index = np.row_stack((src, trg))
    target_arr= target_list
    node_arr= node_list
    
    return edge_index, target_arr, node_arr, G
   
            
def get_stack_traces(event_df, mod_df):
    
    pid_lookup = defaultdict(list)
    
    # extract the stack traces
    for i, f in zip(event_df['PID'].values, event_df['location'].values):
        
        # split along '+' for valid function names
        try:
            fn = f.split("+")
        except:  # catches outliers
            continue
            
        # valid entries only have two sections delimited with "+"
        if len(fn) == 2:
            pid_lookup[i].append(fn[0][:-1])
            
    return pid_lookup
            

if __name__ == '__main__':
    
    parser = argparse.ArgumentParser()
    
    # Config Related
    parser.add_argument('-t', "--event_type", choices=[item.name for item in EventType],
                        help="event type to use, e.g. Reg, File or Thread (default: Reg)", default=EventType.Thread.name)
    parser.add_argument('-e', "--self_edges",
                        help="whether to add self edges to the graph adjacency (default: False)", action='store_true')
    parser.add_argument('-s', "--save",
                        help="whether to save intermediate network config (default: False)", action='store_false')
    parser.add_argument('-p', "--plot",
                        help="whether to plot graph networks (default: False)", action='store_true')
    
    args = parser.parse_args()
    
    # n-gram specific config settings
    ngram_config = {
        "min_df": 2,  # lowest number of entries required to be represented in n-grams
        "ngram": (1) # level of n-gram to use; 1 = unigram, 2 = bigram, 3 = trigram, etc.
    }
    
    # create master config
    master_config = dict()
    for arg in vars(args):
        master_config[arg] = getattr(args, arg)
    
    # wrap n-gram config in master config
    master_config.update(ngram_config)
    
    main(master_config)
