{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from xml.etree import cElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Dataset, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_xml(base_path):\n",
    "    \n",
    "    # loop through .XML files\n",
    "    for filename in os.listdir(base_path):\n",
    "        \n",
    "        # loop through relevent event type xml files\n",
    "        if filename.endswith(f\"{cfg['event_type']}.XML\"):\n",
    "            \n",
    "            # get the full path and root of xml file\n",
    "            path = os.path.join(base_path, 'raw', filename)\n",
    "            root = ET.parse(path).getroot()\n",
    "            \n",
    "            # get the target name of the malware execution\n",
    "            target_name = filename.split(\".\")[-3].lower()\n",
    "            \n",
    "            attr_dict = defaultdict(list)\n",
    "\n",
    "            # root[0] is process, root[1] is event\n",
    "            for child in root[0]:\n",
    "                for event in child:\n",
    "                    tag, text = event.tag, event.text\n",
    "\n",
    "                    # get the module list\n",
    "                    if tag == 'modulelist':\n",
    "\n",
    "                        # build up the module list\n",
    "                        empty_stack = []\n",
    "                        # the path tag is the 4th element in the module list\n",
    "                        for module in event:\n",
    "                            empty_stack.append(module[3].text.split('\\\\')[-1])\n",
    "\n",
    "                        # create the module feature\n",
    "                        attr_dict['modules'].append(' '.join(empty_stack))\n",
    "\n",
    "                    # capture the other keys\n",
    "                    else:\n",
    "                        attr_dict[tag].append(text)\n",
    "                      \n",
    "            # create save dir and save it\n",
    "            save_dir = os.path.join(base_path, 'attributes', cfg[\"event_type\"])\n",
    "            os.makedirs(save_dir, exist_ok=False)\n",
    "            df = pd.DataFrame(attr_dict).to_csv(save_dir + f'\\\\{target_name}.csv')\n",
    "        \n",
    "process_xml(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 1696)\n",
      "(63,)\n",
      "(2, 193)\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def assign_labels(df, file_name):\n",
    "\n",
    "    prev_len = -1\n",
    "    target_name = file_name.split(\".\")[0]\n",
    "\n",
    "    # get the malicious pids of the parent process\n",
    "    mal_pids = set()\n",
    "    df['ProcessName'] = df['ProcessName'].apply(lambda x: x.lower())\n",
    "    mal_pids.add(df[df['ProcessName'] == target_name + '.exe']['ProcessId'].unique()[0])\n",
    "\n",
    "    # stops when the length of pids stops growing\n",
    "    while prev_len != len(mal_pids):\n",
    "        new_rows = df[df['ParentProcessId'].isin(mal_pids)]\n",
    "        new_pids = set(new_rows['ProcessId'].unique())\n",
    "\n",
    "        # updates the lengths and the malicious pids\n",
    "        prev_len = len(mal_pids)\n",
    "        mal_pids.update(new_pids)\n",
    "\n",
    "    df['label'] = df['ProcessId'].apply(lambda x: 1 if x in mal_pids else 0)\n",
    "    \n",
    "    return torch.tensor(df['label'].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "def get_feature_vectors(df, ngram=(2, 2), min_df=1):\n",
    "    \n",
    "    # exposed the args for future tuning\n",
    "    # setting min_df to 1 removes the imports from the filename\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram,\n",
    "                                 lowercase=True,\n",
    "                                 binary=False,\n",
    "                                 min_df=min_df)\n",
    "    # fill nans with space\n",
    "    df['modules'].fillna('', inplace=True)\n",
    "    \n",
    "    return torch.tensor(vectorizer.fit_transform(df['modules']), dtype=torch.float)\n",
    "\n",
    "\n",
    "def get_edge_list(graph_nodes, add_self_edges=True):\n",
    "    \n",
    "    ## Create the adjacency list using nx\n",
    "    G = nx.from_pandas_edgelist(graph_nodes, 'ProcessId', 'ParentProcessId')\n",
    "    \n",
    "    ## Create some empty lists\n",
    "    trg, src = [], []\n",
    "    node_list = []\n",
    "    \n",
    "    ## Loop through the adjacency list\n",
    "    for n, nbrdict in G.adjacency():\n",
    "        for target in nbrdict.keys():\n",
    "            src.append(n)\n",
    "            trg.append(target)\n",
    "            \n",
    "        ## Append the PID and the target\n",
    "        node_list.append(n)\n",
    "\n",
    "    ## Add self-edges if set to true\n",
    "    if add_self_edges:\n",
    "        src.extend(node_list)\n",
    "        trg.extend(node_list)\n",
    "        \n",
    "    ## Convert to numpy arrays [2, E]\n",
    "    edge_index = np.row_stack((src, trg))\n",
    "    \n",
    "    return torch.tensor(edge_index, dtype=torch.int64)\n",
    "\n",
    "\n",
    "## main\n",
    "attr_base = os.path.join(ATTR_PATH, cfg[\"event_type\"])\n",
    "\n",
    "for filename in os.listdir(attr_base):\n",
    "    graph_nodes = pd.read_csv(attr_base + '\\\\' + filename)\n",
    "    \n",
    "    ## Assign labels and return array\n",
    "    target_list = assign_labels(graph_nodes, filename)\n",
    "    \n",
    "    ## Get the processed feature list\n",
    "    feature_list = get_feature_vectors(graph_nodes, ngram=(2, 2), min_df=1)\n",
    "    \n",
    "    ## Get the edge list\n",
    "    edge_list = get_edge_list(graph_nodes)\n",
    "    \n",
    "    # Sanity check for dimensions of feature and target\n",
    "    assert feature_list.shape[0] == len(target_list),\\\n",
    "                f\"Shape mismatch, feature length: {feature_list.shape[0]}  target length: {len(target_list)}\"\n",
    "    \n",
    "    print(feature_list.shape)\n",
    "    print(target_list.shape)\n",
    "    print(edge_list.shape)\n",
    "    print(target_list)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalwareDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, root, cfg, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = where the dataset should be stored\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        \n",
    "        super(MalwareDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # if this file exists in /raw, the download is not triggered\n",
    "        return 'not_implemented.csv'\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # if this file exists in /processed, the download is not triggered\n",
    "        return 'not_implemented.pt'\n",
    "    \n",
    "    # ignore the download helper fn\n",
    "    def download(self): pass\n",
    "\n",
    "    #def len(self):\n",
    "    #    return None\n",
    "\n",
    "    # main process \n",
    "    def process(self):\n",
    "        \n",
    "        # parent list for all the executable graphs\n",
    "        exe_graphs = []\n",
    "        \n",
    "        ## main\n",
    "        attr_base = os.path.join(ATTR_PATH, cfg[\"event_type\"])\n",
    "\n",
    "        for filename in os.listdir(attr_base):\n",
    "            \n",
    "            graph_nodes = pd.read_csv(attr_base + '\\\\' + filename)\n",
    "\n",
    "            ## Assign labels and return array\n",
    "            target_list = assign_labels(graph_nodes, filename)\n",
    "\n",
    "            ## Get the processed feature list\n",
    "            feature_list = get_feature_vectors(graph_nodes, ngram=(2, 2), min_df=1)\n",
    "\n",
    "            ## Get the edge list\n",
    "            edge_list = get_edge_list(graph_nodes)\n",
    "        \n",
    "            data = Data(\n",
    "                x = feature_list,\n",
    "                edge_index = edge_list,\n",
    "                y = target_list\n",
    "            )\n",
    "            \n",
    "            exe_graphs.append(data)\n",
    "            \n",
    "        data, slices = self.collate(exe_graphs)\n",
    "        torch.save((data, slices), 'example.pt')\n",
    "        \n",
    "        \n",
    "cfg = dict(\n",
    "    event_type='Thread'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty master dict for all samples and attributes\n",
    "master_repo = defaultdict(list)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:gat]",
   "language": "python",
   "name": "conda-env-gat-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
