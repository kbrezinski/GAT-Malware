{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import json\n",
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "\n",
    "import optuna\n",
    "from numpyencoder import NumpyEncoder\n",
    "from optuna.integration.mlflow import MLflowCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import GATConv, Linear\n",
    "from torch.functional import F\n",
    "\n",
    "class GATModule(torch.nn.Module):\n",
    "    def __init__(self, in_dims, num_layers, hidden_channels, dropout=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = dropout\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        self.convs.append(GATConv(in_dims, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GATConv(hidden_channels, hidden_channels, concat=False))\n",
    "        self.convs.append(GATConv(hidden_channels, 2, concat=False))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for conv in self.convs:\n",
    "            conv.reset_parameters()\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index).relu()\n",
    "            if self.dropout is not None:\n",
    "                x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return F.log_softmax(x, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def train(args, data, trial=None):\n",
    "\n",
    "    # init models and optimizers\n",
    "    data = MalwareDataset(root='data', cfg=args)[0]\n",
    "\n",
    "    # init models and optimizers\n",
    "    model = GATModule(in_dims=data[0].x.shape[1], num_layers=args['num_layers'], hidden_channels=args['hid'], dropout=args['dropout']).to('cuda:0')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=5e-4)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        for graph in data:\n",
    "            \n",
    "            # unravel batch\n",
    "            x, y, edge_index = \\\n",
    "                graph.x.to('cuda:0'), graph.y.to('cuda:0'), graph.edge_index.to('cuda:0')\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x, edge_index)\n",
    "            loss = F.cross_entropy(logits, y, weight=torch.tensor([1., 2.],).to('cuda:0'))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += float(loss)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_acc += (predictions == y).sum().item() / len(y)\n",
    "\n",
    "        # log metrics\n",
    "        if not epoch % 50:\n",
    "            print(f\"Epoch {epoch:02d} | Loss: {total_loss / len(data):.4f} | Acc: {total_acc / len(data):.4f}\")\n",
    "\n",
    "        if not trial:\n",
    "            mlflow.log_metrics({\"train_loss\": loss.item()}, step=epoch)\n",
    "        \n",
    "        if trial:\n",
    "            trial.report(float(loss.item()), epoch)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        total_loss = 0\n",
    "        total_acc = 0\n",
    "        performances = {'precision': 0, 'recall': 0, 'f1': 0}\n",
    "        for graph in data:\n",
    "            # unravel batch\n",
    "            x, y, edge_index = \\\n",
    "                graph.x.to('cuda:0'), graph.y.to('cuda:0'), graph.edge_index.to('cuda:0')\n",
    "            \n",
    "            logits = model(x, edge_index)\n",
    "            loss = F.cross_entropy(logits, y, weight=torch.tensor([1., 2.],).to('cuda:0'))\n",
    "            total_loss += float(loss)\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            total_acc += (predictions == y).sum().item() / len(y)\n",
    "\n",
    "            performance = precision_recall_fscore_support(\n",
    "                y.cpu(), predictions.cpu(), average='weighted', zero_division=0)\n",
    "            performances['precision'] += performance[0]\n",
    "            performances['recall'] += performance[1]\n",
    "            performances['f1'] += performance[2]\n",
    "\n",
    "        for key in performances.keys():\n",
    "            performances[key] /= len(data)\n",
    "\n",
    "    return {\n",
    "        'args': args,\n",
    "        'model': model,\n",
    "        'performance': performances\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ngram'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39m# Specify arguments\u001b[39;00m\n\u001b[0;32m      4\u001b[0m args \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m      5\u001b[0m     event_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      6\u001b[0m     features\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstack\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      7\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m,\n\u001b[0;32m      8\u001b[0m )\n\u001b[1;32m---> 10\u001b[0m train(args, data, trial\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "Cell \u001b[1;32mIn[20], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, data, trial)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(args, data, trial\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m     \u001b[39m# init models and optimizers\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     data \u001b[39m=\u001b[39m MalwareDataset(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m, cfg\u001b[39m=\u001b[39;49margs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      9\u001b[0m     \u001b[39m# init models and optimizers\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     model \u001b[39m=\u001b[39m GATModule(in_dims\u001b[39m=\u001b[39mdata[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], num_layers\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m], hidden_channels\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mhid\u001b[39m\u001b[39m'\u001b[39m], dropout\u001b[39m=\u001b[39margs[\u001b[39m'\u001b[39m\u001b[39mdropout\u001b[39m\u001b[39m'\u001b[39m])\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32md:\\Code\\Papers\\GAT\\utils\\data_loading.py:25\u001b[0m, in \u001b[0;36mMalwareDataset.__init__\u001b[1;34m(self, root, cfg, transform, pre_transform)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_type \u001b[39m=\u001b[39m cfg[\u001b[39m'\u001b[39m\u001b[39mevent_type\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39m=\u001b[39m transform\n\u001b[1;32m---> 25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mngram \u001b[39m=\u001b[39m cfg[\u001b[39m'\u001b[39;49m\u001b[39mngram\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m     26\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_set \u001b[39m=\u001b[39m cfg[\u001b[39m'\u001b[39m\u001b[39mfeatures\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     28\u001b[0m \u001b[39msuper\u001b[39m(MalwareDataset, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(root, transform, pre_transform)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'ngram'"
     ]
    }
   ],
   "source": [
    "from utils.data_loading import MalwareDataset\n",
    "\n",
    "# Specify arguments\n",
    "args = dict(\n",
    "    event_type='file',\n",
    "    features='stack',\n",
    "    epochs=200,\n",
    ")\n",
    "\n",
    "train(args, data, trial=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/03/07 14:35:12 INFO mlflow.tracking.fluent: Experiment with name 'baselines' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='file:///d:\\\\Code\\\\Papers\\\\GAT\\\\experiments/117438263073752970', creation_time=1678221312330, experiment_id='117438263073752970', last_update_time=1678221312330, lifecycle_stage='active', name='baselines', tags={}>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_experiment(experiment_name=\"baselines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set tracking URI\n",
    "MODEL_REGISTRY = Path(\"experiments\")\n",
    "Path(MODEL_REGISTRY).mkdir(exist_ok=True) # create experiments dir\n",
    "mlflow.set_tracking_uri(\"file:///\" + str(MODEL_REGISTRY.absolute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.5179 | Acc: 0.9191\n",
      "Epoch 50 | Loss: 0.2225 | Acc: 0.9238\n",
      "Epoch 100 | Loss: 0.2218 | Acc: 0.9375\n",
      "Epoch 150 | Loss: 0.2317 | Acc: 0.9330\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import tempfile\n",
    "\n",
    "def save_dict(d, path):\n",
    "    with open(path, 'w') as fp:\n",
    "        json.dump(d, indent=2, sort_keys=False, fp=fp)\n",
    "\n",
    "# Tracking\n",
    "with mlflow.start_run(run_name=\"gnn\"):\n",
    "\n",
    "    # Train & evaluate\n",
    "    artifacts = train(args=args, data=data)\n",
    "\n",
    "    # Log key metrics\n",
    "    mlflow.log_metrics({\"precision\": artifacts[\"performance\"][\"precision\"]})\n",
    "    mlflow.log_metrics({\"recall\": artifacts[\"performance\"][\"recall\"]})\n",
    "    mlflow.log_metrics({\"f1\": artifacts[\"performance\"][\"f1\"]})\n",
    "\n",
    "    # Log artifacts\n",
    "    with tempfile.TemporaryDirectory() as dp:\n",
    "        joblib.dump(artifacts[\"model\"], Path(dp, \"model.pkl\"))\n",
    "        save_dict(artifacts[\"performance\"], Path(dp, \"performance.json\"))\n",
    "        mlflow.log_artifacts(dp)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_params(artifacts[\"args\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(args, trial):\n",
    "    \"\"\"Objective function for optimization trials.\"\"\"\n",
    "    # Parameters to tune\n",
    "    args['dropout'] = trial.suggest_categorical(\"dropout\", [None, 0.2, 0.5])\n",
    "    args['ngram'] = trial.suggest_categorical(\"ngram\", [(1, 1), (2, 2), (3, 3)])\n",
    "    args['lr'] = trial.suggest_loguniform(\"lr\", 1e-5, 1e-2)\n",
    "    args['hid'] = trial.suggest_int(\"hid\", 16, 128, step=16)\n",
    "    args['num_layers'] = trial.suggest_int(\"hid\", 2, 4)\n",
    "\n",
    "    # Train & evaluate\n",
    "    artifacts = train(args=args, data=data, trial=trial)\n",
    "\n",
    "    # Set additional attributes\n",
    "    performance = artifacts[\"performance\"]\n",
    "    print(json.dumps(performance, indent=2))\n",
    "    trial.set_user_attr(\"precision\", performance[\"precision\"])\n",
    "    trial.set_user_attr(\"recall\", performance[\"recall\"])\n",
    "    trial.set_user_attr(\"f1\", performance[\"f1\"])\n",
    "\n",
    "    return performance[\"f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 14:42:13,983]\u001b[0m A new study created in memory with name: optimization\u001b[0m\n",
      "C:\\Users\\Kenneth\\AppData\\Local\\Temp\\ipykernel_22276\\2368015346.py:6: ExperimentalWarning: MLflowCallback is experimental (supported from v1.4.0). The interface can change in the future.\n",
      "  mlflow_callback = MLflowCallback(\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (1, 1) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (2, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (3, 3) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\trial\\_trial.py:772: RuntimeWarning: Inconsistent parameter values for distribution with name \"hid\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 16, 'high': 128, 'step': 16}\n",
      "  warnings.warn(\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .XML files...\n",
      "Processing ardamax.file.csv...\n",
      "Processing asprox.file.csv...\n",
      "Processing cerber.file.csv...\n",
      "Processing cryptowall.file.csv...\n",
      "Processing dyre.file.csv...\n",
      "Processing grayfish.file.csv...\n",
      "Processing kelihos.file.csv...\n",
      "Processing rustock23.file.csv...\n",
      "Processing rustocki.file.csv...\n",
      "Processing rustockj.file.csv...\n",
      "Processing shamoon.file.csv...\n",
      "Processing wannacry.file.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.6430 | Acc: 0.9129\n",
      "Epoch 50 | Loss: 0.3958 | Acc: 0.9275\n",
      "Epoch 100 | Loss: 0.3982 | Acc: 0.9275\n",
      "Epoch 150 | Loss: 0.3884 | Acc: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 14:48:00,232]\u001b[0m Trial 0 finished with value: 0.8933808477144652 and parameters: {'dropout': 0.5, 'ngram': (3, 3), 'lr': 0.0019479015040492164, 'hid': 48}. Best is trial 0 with value: 0.8933808477144652.\u001b[0m\n",
      "2023/03/07 14:48:00 INFO mlflow.tracking.fluent: Experiment with name 'optimization' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.8630183775267972,\n",
      "  \"recall\": 0.9274645353452454,\n",
      "  \"f1\": 0.8933808477144652\n",
      "}\n",
      "Processing .XML files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (1, 1) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (2, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (3, 3) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\trial\\_trial.py:772: RuntimeWarning: Inconsistent parameter values for distribution with name \"hid\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 16, 'high': 128, 'step': 16}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.6905 | Acc: 0.8629\n",
      "Epoch 50 | Loss: 0.3610 | Acc: 0.9275\n",
      "Epoch 100 | Loss: 0.2838 | Acc: 0.9275\n",
      "Epoch 150 | Loss: 0.2504 | Acc: 0.9099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 14:49:57,711]\u001b[0m Trial 1 finished with value: 0.8942760543468548 and parameters: {'dropout': None, 'ngram': (3, 3), 'lr': 0.00031365662902328734, 'hid': 16}. Best is trial 1 with value: 0.8942760543468548.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.886739206438894,\n",
      "  \"recall\": 0.9080819502150462,\n",
      "  \"f1\": 0.8942760543468548\n",
      "}\n",
      "Processing .XML files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (1, 1) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (2, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (3, 3) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\trial\\_trial.py:772: RuntimeWarning: Inconsistent parameter values for distribution with name \"hid\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 16, 'high': 128, 'step': 16}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.6925 | Acc: 0.9275\n",
      "Epoch 50 | Loss: 0.4018 | Acc: 0.9275\n",
      "Epoch 100 | Loss: 0.4075 | Acc: 0.9275\n",
      "Epoch 150 | Loss: 0.3859 | Acc: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 14:57:33,431]\u001b[0m Trial 2 finished with value: 0.8933808477144652 and parameters: {'dropout': 0.2, 'ngram': (3, 3), 'lr': 2.2171615547483988e-05, 'hid': 64}. Best is trial 1 with value: 0.8942760543468548.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.8630183775267972,\n",
      "  \"recall\": 0.9274645353452454,\n",
      "  \"f1\": 0.8933808477144652\n",
      "}\n",
      "Processing .XML files...\n",
      "Processing ardamax.file.csv...\n",
      "Processing asprox.file.csv...\n",
      "Processing cerber.file.csv...\n",
      "Processing cryptowall.file.csv...\n",
      "Processing dyre.file.csv...\n",
      "Processing grayfish.file.csv...\n",
      "Processing kelihos.file.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (1, 1) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (2, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (3, 3) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\trial\\_trial.py:772: RuntimeWarning: Inconsistent parameter values for distribution with name \"hid\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 16, 'high': 128, 'step': 16}\n",
      "  warnings.warn(\n",
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing rustock23.file.csv...\n",
      "Processing rustocki.file.csv...\n",
      "Processing rustockj.file.csv...\n",
      "Processing shamoon.file.csv...\n",
      "Processing wannacry.file.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.6915 | Acc: 0.9275\n",
      "Epoch 50 | Loss: 0.3886 | Acc: 0.9275\n",
      "Epoch 100 | Loss: 0.3885 | Acc: 0.9275\n",
      "Epoch 150 | Loss: 0.3885 | Acc: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 15:05:15,772]\u001b[0m Trial 3 finished with value: 0.8933808477144652 and parameters: {'dropout': None, 'ngram': (2, 2), 'lr': 4.942203969251597e-05, 'hid': 64}. Best is trial 1 with value: 0.8942760543468548.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.8630183775267972,\n",
      "  \"recall\": 0.9274645353452454,\n",
      "  \"f1\": 0.8933808477144652\n",
      "}\n",
      "Processing .XML files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (1, 1) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (2, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (3, 3) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\trial\\_trial.py:772: RuntimeWarning: Inconsistent parameter values for distribution with name \"hid\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 16, 'high': 128, 'step': 16}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.6891 | Acc: 0.9248\n",
      "Epoch 50 | Loss: 0.4087 | Acc: 0.9275\n",
      "Epoch 100 | Loss: 0.3827 | Acc: 0.9275\n",
      "Epoch 150 | Loss: 0.3948 | Acc: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-07 15:07:09,613]\u001b[0m Trial 4 finished with value: 0.8933808477144652 and parameters: {'dropout': 0.5, 'ngram': (2, 2), 'lr': 0.000195647254674554, 'hid': 16}. Best is trial 1 with value: 0.8942760543468548.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"precision\": 0.8630183775267972,\n",
      "  \"recall\": 0.9274645353452454,\n",
      "  \"f1\": 0.8933808477144652\n",
      "}\n",
      "Processing .XML files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (1, 1) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (2, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\distributions.py:427: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (3, 3) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\trial\\_trial.py:772: RuntimeWarning: Inconsistent parameter values for distribution with name \"hid\"! This might be a configuration mistake. Optuna allows to call the same distribution with the same name more then once in a trial. When the parameter values are inconsistent optuna only uses the values of the first call and ignores all following. Using these values: {'low': 16, 'high': 128, 'step': 16}\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | Loss: 0.4997 | Acc: 0.9275\n",
      "Epoch 50 | Loss: 0.3951 | Acc: 0.9275\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(study_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptimization\u001b[39m\u001b[39m\"\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m, pruner\u001b[39m=\u001b[39mpruner)\n\u001b[0;32m      6\u001b[0m mlflow_callback \u001b[39m=\u001b[39m MLflowCallback(\n\u001b[0;32m      7\u001b[0m     tracking_uri\u001b[39m=\u001b[39mmlflow\u001b[39m.\u001b[39mget_tracking_uri(), metric_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m study\u001b[39m.\u001b[39;49moptimize(\u001b[39mlambda\u001b[39;49;00m trial: objective(args, trial),\n\u001b[0;32m      9\u001b[0m                 n_trials\u001b[39m=\u001b[39;49mNUM_TRIALS,\n\u001b[0;32m     10\u001b[0m                 callbacks\u001b[39m=\u001b[39;49m[mlflow_callback])\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\study\\study.py:400\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39mif\u001b[39;00m n_jobs \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    393\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    394\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`n_jobs` argument has been deprecated in v2.7.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThis feature will be removed in v4.0.0. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    396\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSee https://github.com/optuna/optuna/releases/tag/v2.7.0.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    397\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    398\u001b[0m     )\n\u001b[1;32m--> 400\u001b[0m _optimize(\n\u001b[0;32m    401\u001b[0m     study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    402\u001b[0m     func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    403\u001b[0m     n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    404\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    405\u001b[0m     n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    406\u001b[0m     catch\u001b[39m=\u001b[39;49mcatch,\n\u001b[0;32m    407\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    408\u001b[0m     gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    409\u001b[0m     show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    410\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[0;32m     67\u001b[0m             study,\n\u001b[0;32m     68\u001b[0m             func,\n\u001b[0;32m     69\u001b[0m             n_trials,\n\u001b[0;32m     70\u001b[0m             timeout,\n\u001b[0;32m     71\u001b[0m             catch,\n\u001b[0;32m     72\u001b[0m             callbacks,\n\u001b[0;32m     73\u001b[0m             gc_after_trial,\n\u001b[0;32m     74\u001b[0m             reseed_sampler_rng\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m     75\u001b[0m             time_start\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m     76\u001b[0m             progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m     77\u001b[0m         )\n\u001b[0;32m     78\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[39mif\u001b[39;00m show_progress_bar:\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\optuna\\study\\_optimize.py:213\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    210\u001b[0m     thread\u001b[39m.\u001b[39mstart()\n\u001b[0;32m    212\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 213\u001b[0m     value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[26], line 8\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      5\u001b[0m study \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(study_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39moptimization\u001b[39m\u001b[39m\"\u001b[39m, direction\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmaximize\u001b[39m\u001b[39m\"\u001b[39m, pruner\u001b[39m=\u001b[39mpruner)\n\u001b[0;32m      6\u001b[0m mlflow_callback \u001b[39m=\u001b[39m MLflowCallback(\n\u001b[0;32m      7\u001b[0m     tracking_uri\u001b[39m=\u001b[39mmlflow\u001b[39m.\u001b[39mget_tracking_uri(), metric_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m study\u001b[39m.\u001b[39moptimize(\u001b[39mlambda\u001b[39;00m trial: objective(args, trial),\n\u001b[0;32m      9\u001b[0m                 n_trials\u001b[39m=\u001b[39mNUM_TRIALS,\n\u001b[0;32m     10\u001b[0m                 callbacks\u001b[39m=\u001b[39m[mlflow_callback])\n",
      "Cell \u001b[1;32mIn[25], line 11\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(args, trial)\u001b[0m\n\u001b[0;32m      8\u001b[0m args[\u001b[39m'\u001b[39m\u001b[39mnum_layers\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m trial\u001b[39m.\u001b[39msuggest_int(\u001b[39m\"\u001b[39m\u001b[39mhid\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[39m# Train & evaluate\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m artifacts \u001b[39m=\u001b[39m train(args\u001b[39m=\u001b[39;49margs, data\u001b[39m=\u001b[39;49mdata, trial\u001b[39m=\u001b[39;49mtrial)\n\u001b[0;32m     13\u001b[0m \u001b[39m# Set additional attributes\u001b[39;00m\n\u001b[0;32m     14\u001b[0m performance \u001b[39m=\u001b[39m artifacts[\u001b[39m\"\u001b[39m\u001b[39mperformance\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "Cell \u001b[1;32mIn[20], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, data, trial)\u001b[0m\n\u001b[0;32m     23\u001b[0m logits \u001b[39m=\u001b[39m model(x, edge_index)\n\u001b[0;32m     24\u001b[0m loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mcross_entropy(logits, y, weight\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor([\u001b[39m1.\u001b[39m, \u001b[39m2.\u001b[39m],)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda:0\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m---> 25\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     28\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_TRIALS = 20  # small sample for now\n",
    "\n",
    "# Optimize\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=5)\n",
    "study = optuna.create_study(study_name=\"optimization\", direction=\"maximize\", pruner=pruner)\n",
    "mlflow_callback = MLflowCallback(\n",
    "    tracking_uri=mlflow.get_tracking_uri(), metric_name=\"f1\")\n",
    "study.optimize(lambda trial: objective(args, trial),\n",
    "                n_trials=NUM_TRIALS,\n",
    "                callbacks=[mlflow_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
