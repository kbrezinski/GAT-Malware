
import torch
import numpy as np

from torch import nn


class MassFD(nn.Module):
    '''
    Determine the mass radius or radius of gyration fractal dimension for a torch implementation. Same params as the vanilla
    implementation, but removed some experimental settings
    
    args:
        x        - (n, m) feature vector. Can be any m-th dimensional data, with n datapoints
        k        - number of scales to use
        skip     - skip some points on the log-log plot. Helps on occasion for numerical stability in the upper/lower range
        gyration - flag to enable radius of gyration instead of mass-radius
        verbose  - plot the log-log plot and print some metadata
        
    returns:
        D        - returns the fractal dimension of the input
    '''
    
    def __init__(self, k=5, skip=0, gyration=True, use_weight=False, norm=2., fd_diff=False, **kwargs):
        super().__init__()
        
        assert k - skip >= 2, f"Not enough points on the log-log plot to consider, you used: {k=} and {skip=} but need at least 2"
        
        self.k = k
        self.skip = skip
        self.gyration = gyration
        self.fd_diff = fd_diff

        # create the trainable norm parameter
        if norm is None:
            self.norm = nn.Parameter(torch.Tensor([2.]))
        else:
            self.norm = norm
                 
        # create the bias layer
        if use_weight:
            self.weight = nn.Parameter(torch.ones(1))
        else:
            self.register_parameter('weight', None)
        
    def extra_repr(self):
        return 'k={}, skip={}, gyration={}, use_weight={}, fd_diff={}'.format(
            self.k, self.skip, self.gyration,
            True if self.weight is not None else False,
            self.fd_diff
        )

    def calculate_fd(self, x):

        # remove zero values; useful for numeric stability when n_dim is low
        x = x[~torch.all(x == 0., axis=1)]

        # mean along each axis
        centroid = x.mean(axis=0) 

        # find distances and sort by proximity to centroid       
        dist = torch.pow(torch.sum(torch.abs(x - centroid) ** self.norm, axis=1), 1. / self.norm)
        dist_idx_sorted = torch.argsort(dist)

        # split distances array into k partitions
        N = np.array_split(dist_idx_sorted, self.k)
        # points for log-log plot
        Rg, Nk = [], []

        # loop through scales
        for i in range(self.k):

            # concatenates sucessive nodes away from centroid (e.g Nk_1, Nk_1 + Nk_2)
            Nj = x[torch.cat(N[:i+1])]

            # compute the centroid for each k if using ROG
            if self.gyration:
                centroid = torch.mean(Nj, axis=0)

            # compute the std. dev in all d dimensions; can use mean instead of sum
            Rg.append(torch.pow(torch.sum(torch.abs(Nj - centroid) ** self.norm) / len(Nj), 1. / self.norm))
            # append the number of nodes considered
            Nk.append(Nj.shape[0])

        # convert lists to tensors to make them (k, 1) matrices
        Rg = torch.Tensor(Rg).unsqueeze(-1)
        Nk = torch.Tensor(Nk).unsqueeze(-1)

        # vanilla implementation, just used the raw std. devs.
        log_Rg = torch.log(Rg)[:-self.skip] if self.skip != 0 else torch.log(Rg)
        log_Nk = torch.log(Nk)[:-self.skip] if self.skip != 0 else torch.log(Nk)

        # closed-form least squares linear regression (X.T X)^-1 X.T y
        D = torch.matmul(torch.matmul(
            torch.cholesky_inverse(torch.matmul(log_Rg.T, log_Rg)), log_Rg.T), log_Nk)\
            .squeeze(-1)

        return D
        

    def forward(self, x, y):

        # calculate on whole sample set
        fd = self.calculate_fd(x)

        # determine fd on only negative samples; then diff
        if self.fd_diff:
            neg_samples = x[y == 0, :]
            fd -= self.calculate_fd(neg_samples)
            
        # returns scalar with or without weight
        return torch.mul(self.weight, fd) if self.weight is not None else fd