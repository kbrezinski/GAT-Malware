
import torch
import numpy as np

from torch import nn


class MassFD(nn.Module):
    '''
    Determine the mass radius or radius of gyration fractal dimension for a torch implementation. Same params as the vanilla
    implementation, but removed some experimental settings
    
    args:
        x        - (n, m) feature vector. Can be any m-th dimensional data, with n datapoints
        k        - number of scales to use
        skip     - skip some points on the log-log plot. Helps on occasion for numerical stability in the upper/lower range
        gyration - flag to enable radius of gyration instead of mass-radius
        verbose  - plot the log-log plot and print some metadata
        
    returns:
        D        - returns the fractal dimension of the input
    '''
    
    def __init__(self, out_dim, k=5, skip=0, gyration=True, use_weight=False, norm=2.):
        super().__init__()
        
        assert k - skip >= 2, f"Not enough points on the log-log plot to consider, you used: {k=} and {skip=} but need at least 2"
        
        self.out_dim = out_dim
        self.k = k
        self.skip = skip
        self.gyration = gyration

        # create the trainable norm parameter
        if norm is None:
            self.norm = nn.Parameter(torch.Tensor([2.]))
        else:
            self.norm = norm
                 
        # create the bias layer
        if use_weight:
            self.weight = nn.Parameter(torch.ones((out_dim,)))
        else:
            self.register_parameter('weight', None)
        
    def extra_repr(self):
        return 'out_dim={}, k={}, skip={}, gyration={}, use_weight={}'.format(
            self.out_dim, self.k, self.skip, self.gyration,
            True if self.weight is not None else False
        )

    def :
        
        
    def forward(self, x):
        
        # remove zero values; useful for numeric stability when n_dim is low
        x = x[~torch.all(x == 0., axis=1)]

        # mean along each axis
        centroid = x.mean(axis=0) 

        # find distances and sort by proximity to centroid       
        dist = torch.pow(torch.sum(torch.abs(x - centroid) ** self.norm, axis=1), 1. / self.norm)
        dist_idx_sorted = torch.argsort(dist)

        # split distances array into k partitions
        N = np.array_split(dist_idx_sorted, self.k)
        # points for log-log plot
        Rg, Nk = [], []

        # loop through scales
        for i in range(self.k):

            # concatenates sucessive nodes away from centroid (e.g Nk_1, Nk_1 + Nk_2)
            Nj = x[torch.cat(N[:i+1])]

            # compute the centroid for each k if using ROG
            if self.gyration:
                centroid = torch.mean(Nj, axis=0)

            # compute the std. dev in all d dimensions; can use mean instead of sum
            Rg.append(torch.pow(torch.sum(torch.abs(Nj - centroid) ** self.norm) / len(Nj), 1. / self.norm))
            # append the number of nodes considered
            Nk.append(Nj.shape[0])

        # convert lists to tensors to make them (k, 1) matrices
        Rg = torch.Tensor(Rg).unsqueeze(-1).requires_grad_()
        Nk = torch.Tensor(Nk).unsqueeze(-1).requires_grad_()

        # vanilla implementation, just used the raw std. devs.
        log_Rg = torch.log(Rg)[:-self.skip] if self.skip != 0 else torch.log(Rg)
        log_Nk = torch.log(Nk)[:-self.skip] if self.skip != 0 else torch.log(Nk)

        # closed-form least squares linear regression (X.T X)^-1 X.T y
        D = torch.matmul(torch.matmul(
            torch.cholesky_inverse(torch.matmul(log_Rg.T, log_Rg)), log_Rg.T), log_Nk)\
            .squeeze(-1)

        # returns size (out_dim,) with or without weight
        return torch.mul(self.weight, D) if self.weight is not None \
                else D.repeat(self.out_dim)