
import torch
import numpy as np

from torch import nn
from torch_geometric.nn import GCNConv, GATConv

class ComplexityGNN(torch.nn.Module):

    def __init__(self, cfg):
        super(ComplexityGNN, self).__init__()

        layer_dims = cfg['arch']
        fd_layers = [] if not cfg['use_fd'] else cfg['fd_layers']
        layer_type = cfg['layer_type']

        # create the layers
        self.layers = nn.ModuleList()
        for i, (in_d, out_d) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):

            # append fd layer if needed
            if i in fd_layers:
                self.layers.append(MassFD(**cfg))

            # append layers
            self.layers.append(layer_type(in_d, out_d, heads=1))
            
            # append activation
            self.layers.append(nn.ReLU(inplace=True))
        
        # append final fd layer if needed
        if fd_layers and fd_layers[-1] == len(layer_dims) - 1:
            self.layers.append(MassFD(**cfg))

        self.layers.append(nn.Linear(out_d, cfg['num_classes']))


    def forward(self, x, y, edge_index, writer=None, epoch=None):

        # forward pass through the layers
        for i, layer in enumerate(self.layers):
            if isinstance(layer, (GCNConv, GATConv)):
                x = layer(x, edge_index)
            elif isinstance(layer, (nn.ReLU, nn.Linear)):
                x = layer(x)
            else:
                fd = layer(x)
                x = x + fd

                # log value/weight
                if writer is not None:
                    writer.add_scalar(f'layer{i}.fd_value', fd, epoch)    
        return x


class MassFD(nn.Module):
    '''
    Determine the mass radius or radius of gyration fractal dimension for a torch implementation. Same params as the vanilla
    implementation, but removed some experimental settings
    
    args:
        k          - number of scales to use
        skip       - skip some points on the log-log plot. Helps on occasion for numerical stability in the upper/lower range
        gyration   - flag to enable radius of gyration instead of mass-radius
        use_weight - flag to enable a trainable weight for the FD
        norm       - norm to use for the minkowski distance
        device     - device to use for the FD
        
    returns:
        D          - returns the fractal dimension of the input
    '''
    
    def __init__(self, k=5, skip=0, gyration=True, norm=2., device='cuda:0', **kwargs):
        super().__init__()
        
        # check that there are enough points on the log-log plot to consider
        assert k - skip >= 2, f"Not enough points on the log-log plot to consider, \
                                you used: {k=} and {skip=} but need at least 2"
        
        self.k = k
        self.skip = skip
        self.gyration = gyration
        self.fd_diff = False # always disabled
        self.use_weight = True # always enabled
        self.device = device

        # create the trainable norm parameter 
        self.norm = nn.Parameter(
            torch.ones(1, device=device), requires_grad=False) \
            if norm is None else norm
                 
        # create the weight layer
        if self.use_weight:
            self.weight = nn.Parameter(torch.rand(1, device=device))
        else:
            self.register_parameter('weight', None)
    
    # print out the settings
    def extra_repr(self):
        return 'k={}, skip={}, gyration={}, use_weight={}, fd_diff={}'.format(
            self.k, self.skip, self.gyration,
            True if self.weight is not None else False,
            self.fd_diff
        )

    def calculate_fd(self, x):

        # remove zero values; useful for numeric stability when n_dim is low
        x = x[~torch.all(x == 0., axis=1)]

        # mean along each axis
        centroid = x.mean(axis=0) 

        # find distances and sort by proximity to centroid       
        dist = torch.pow(torch.sum(torch.abs(x - centroid) ** self.norm, axis=1), 1. / self.norm)
        dist_idx_sorted = torch.argsort(dist)

        # split distances array into k partitions
        N = np.array_split(dist_idx_sorted, self.k)
        # points for log-log plot
        Rg, Nk = [], []

        # loop through scales
        for i in range(self.k):

            # concatenates sucessive nodes away from centroid (e.g Nk_1, Nk_1 + Nk_2)
            Nj = x[torch.cat(N[:i+1])]

            # compute the centroid for each k if using ROG
            if self.gyration:
                centroid = torch.mean(Nj, axis=0)

            # compute the std. dev in all d dimensions; can use mean instead of sum
            Rg.append(torch.pow(torch.sum(torch.abs(Nj - centroid) ** self.norm) / len(Nj), 1. / self.norm))
            # append the number of nodes considered
            Nk.append(Nj.shape[0])

        # convert lists to tensors to make them (k, 1) matrices
        Rg = torch.Tensor(Rg).unsqueeze(-1)
        Nk = torch.Tensor(Nk).unsqueeze(-1)

        # vanilla implementation, just used the raw std. devs.
        log_Rg = torch.log(Rg)[:-self.skip] if self.skip != 0 else torch.log(Rg)
        log_Nk = torch.log(Nk)[:-self.skip] if self.skip != 0 else torch.log(Nk)

        # closed-form least squares linear regression (X.T X)^-1 X.T y
        D = torch.matmul(torch.matmul(
            torch.cholesky_inverse(torch.matmul(log_Rg.T, log_Rg)), log_Rg.T), log_Nk)\
            .squeeze(-1)

        return D
        

    def forward(self, x):

        # calculate on whole sample set
        fd = self.calculate_fd(x).to(self.device)

        # determine fd on only negative samples; then diff
        # if self.fd_diff:
        #    neg_samples = x[y == 0, :]
        #    fd -= self.calculate_fd(neg_samples)
            
        # returns scalar with or without weight
        return torch.mul(self.weight, fd) if self.weight is not None else fd