{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generates the torch.data version of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from xml.etree import cElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .XML files...\n",
      "anki.file.csv already processed, skipping...\n",
      "slack.file.csv already processed, skipping...\n"
     ]
    }
   ],
   "source": [
    "def process_xml(base_path):\n",
    "\n",
    "    print(\"Processing .XML files...\")\n",
    "\n",
    "    # append raw path to base_dir\n",
    "    XML_PATH = os.path.join(base_path, 'xml')\n",
    "    event_type = cfg['event_type']\n",
    "    \n",
    "    # loop through .XML files\n",
    "    for filename in os.listdir(XML_PATH):\n",
    "        \n",
    "        file_key_name = filename.rsplit(\".\", 1)[0] + '.csv'\n",
    "\n",
    "        if file_key_name in os.listdir(\n",
    "                os.path.join(XML_PATH, os.pardir, 'raw')):\n",
    "            print(f\"{file_key_name} already processed, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # loop through relevent event type xml files\n",
    "        if filename.endswith(f\"{event_type}.XML\"):\n",
    "            \n",
    "            # get the full path and root of xml file\n",
    "            path = os.path.join(base_path, 'xml', filename)\n",
    "            root = ET.parse(path).getroot()\n",
    "            \n",
    "            # get the target name of the malware executable\n",
    "            target_name = filename.split(\".\")[0].lower()\n",
    "            \n",
    "            attr_dict = defaultdict(list)\n",
    "\n",
    "            # root[0] is process, root[1] is event\n",
    "            for child in root[0]:\n",
    "                for event in child:\n",
    "                    tag, text = event.tag, event.text\n",
    "\n",
    "                    # get the module list\n",
    "                    if tag == 'modulelist':\n",
    "\n",
    "                        # build up the module list\n",
    "                        empty_stack = []\n",
    "                        # the path tag is the 4th element in the module list\n",
    "                        for module in event:\n",
    "                            empty_stack.append(module[3].text.split('\\\\')[-1])\n",
    "\n",
    "                        # create the module feature\n",
    "                        attr_dict['modules'].append(' '.join(empty_stack))\n",
    "\n",
    "                    # capture the other keys\n",
    "                    else:\n",
    "                        attr_dict[tag].append(text)\n",
    "                      \n",
    "            # create save dir and save it\n",
    "            save_dir = os.path.join(base_path, 'raw')\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            save_name = f'{target_name}.{event_type}.csv'\n",
    "            df = pd.DataFrame(attr_dict).to_csv(\n",
    "                os.path.join(save_dir, save_name), index=False)\n",
    "            print(f\"Finished saving {save_name} to {save_dir}\")\n",
    "\n",
    "cfg = dict(\n",
    "    event_type = 'file',\n",
    ")    \n",
    "process_xml(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "def assign_labels(df, file_name):\n",
    "\n",
    "    prev_len = -1\n",
    "    target_name = file_name.split(\".\")[0]\n",
    "\n",
    "    # get the malicious pids of the parent process\n",
    "    df['ProcessName'] = df['ProcessName'].apply(lambda x: x.lower())\n",
    "    unique_pids = df[df['ProcessName'] == target_name + '.exe']['ProcessId'].unique()\n",
    "\n",
    "    assert len(unique_pids) > 0, f\"Target name not found in {file_name}\"\n",
    "    mal_pids = set(unique_pids)\n",
    "\n",
    "    # stops when the length of pids stops growing\n",
    "    while prev_len != len(mal_pids):\n",
    "        new_rows = df[df['ParentProcessId'].isin(mal_pids)]\n",
    "        new_pids = set(new_rows['ProcessId'].unique())\n",
    "\n",
    "        # updates the lengths and the malicious pids\n",
    "        prev_len = len(mal_pids)\n",
    "        mal_pids.update(new_pids)\n",
    "\n",
    "    df['label'] = df['ProcessId'].apply(lambda x: 1 if x in mal_pids else 0)\n",
    "    \n",
    "    return torch.tensor(df['label'].values, dtype=torch.long)\n",
    "\n",
    "\n",
    "def get_edge_list(graph_nodes, add_self_edges=True):\n",
    "\n",
    "    ## Create the adjacency list using nx\n",
    "    G = nx.from_pandas_edgelist(graph_nodes, 'ProcessId', 'ParentProcessId')\n",
    "    \n",
    "    ## Create some empty lists\n",
    "    trg, src = [], []\n",
    "    pid_lookup = dict()\n",
    "    \n",
    "    ## Loop through the adjacency list\n",
    "    for n, (source, nbrdict) in enumerate(G.adjacency()):\n",
    "\n",
    "        if source not in pid_lookup:\n",
    "            pid_lookup[source] = len(pid_lookup)\n",
    "\n",
    "        for target in nbrdict.keys():\n",
    "            if target not in pid_lookup:\n",
    "                pid_lookup[target] = len(pid_lookup)\n",
    "\n",
    "            src.append(pid_lookup[source])\n",
    "            trg.append(pid_lookup[target])\n",
    "            \n",
    "    ## Add self-edges if set to true\n",
    "    if add_self_edges:\n",
    "        src.extend(list(range(len(pid_lookup))))\n",
    "        trg.extend(list(range(len(pid_lookup))))\n",
    "        \n",
    "    ## Convert to numpy arrays [2, E]\n",
    "    edge_index = np.row_stack((src, trg))\n",
    "    return torch.tensor(edge_index, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions for anki.file.csv:\n",
      "\t ('anki.file.csv', Data(edge_index=[2, 768], y=[263], x=[263, 4430]))\n",
      "Dimensions for slack.file.csv:\n",
      "\t ('slack.file.csv', Data(edge_index=[2, 712], y=[243], x=[243, 4430]))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Data(edge_index=[2, 768], y=[263], x=[263, 4430]),\n",
       " Data(edge_index=[2, 712], y=[243], x=[243, 4430])]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MalwareDataset(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, root, cfg, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = where the dataset should be stored\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.event_type = cfg['event_type']\n",
    "        \n",
    "        super(MalwareDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        XML_PATH = os.path.join(self.root, 'xml')\n",
    "        # if these files do not exists in /raw, the xml parse is not triggered\n",
    "        return [filename.rsplit('.', 1)[0] + '.csv' for filename in os.listdir(XML_PATH)]\n",
    "        \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # if this file exists in /processed, process is not triggered\n",
    "        return [self.event_type + '.pt']\n",
    "    \n",
    "    def download(self):\n",
    "        # process the raw xml files if csv files are not present\n",
    "        process_xml(self.root)\n",
    "\n",
    "    def len(self):\n",
    "        # only a single list of graphs is stored\n",
    "        return 1\n",
    "\n",
    "    # main process \n",
    "    def process(self):\n",
    "        \n",
    "        # parent list for all the executable graphs\n",
    "        exe_graphs = []\n",
    "        module_list = []\n",
    "\n",
    "        # loop through all the files and read csv\n",
    "        for filename in os.listdir(self.raw_dir):\n",
    "            graph_nodes = pd.read_csv(self.raw_dir + '\\\\' + filename)\n",
    "\n",
    "            # Assign labels and return array\n",
    "            target_list = assign_labels(graph_nodes, filename)\n",
    "            # Get the processed feature list\n",
    "            module_list.append(graph_nodes['modules'])\n",
    "            # Get the edge list\n",
    "            edge_list = get_edge_list(graph_nodes)\n",
    "\n",
    "            # Create the data object\n",
    "            data = Data(\n",
    "                edge_index = edge_list,\n",
    "                y = target_list\n",
    "            )\n",
    "            # append to master list\n",
    "            exe_graphs.append((filename, data))\n",
    "\n",
    "        # get the tfidf vectors for the modules\n",
    "        tfidf_modules =  get_feature_vectors(module_list, ngram=(2, 2), min_df=1)\n",
    "        \n",
    "        for graph, module in zip(exe_graphs, tfidf_modules):\n",
    "            graph[1].x = module\n",
    "            graph[1].edge_index = truncate_edges(graph[1].edge_index, graph[1].x.shape[0])\n",
    "            print(f\"Dimensions for {graph[0]}:\\n\\t {graph}\")\n",
    "        \n",
    "        # collate the list and save the data\n",
    "        torch.save([exe[1] for exe in exe_graphs], self.processed_paths[0])\n",
    "\n",
    "    # fetch the data from the .pt file\n",
    "    def get(self, idx):\n",
    "        return torch.load(self.processed_paths[0])\n",
    "\n",
    "\n",
    "# base config dictionary\n",
    "cfg = dict(\n",
    "    event_type='file',\n",
    "    generate_graph=True,\n",
    ")\n",
    "\n",
    "\n",
    "def truncate_edges(edge_index, max_nodes):\n",
    "    # truncate edges to max_nodes: [2, E]\n",
    "    edge_index = edge_index[:, edge_index[0] < max_nodes]\n",
    "    edge_index = edge_index[:, edge_index[1] < max_nodes]\n",
    "    return edge_index\n",
    "\n",
    "\n",
    "# carries out tfidf vectorization on the modules per sample\n",
    "def get_feature_vectors(module_list, ngram=(2, 2), min_df=1):\n",
    "    \n",
    "    new_list = []\n",
    "    module_shapes = []\n",
    "    # fill nans with space and collect sizes\n",
    "    for module in module_list:\n",
    "        module_shapes.append(module.shape[0])\n",
    "        new_list.append(module.fillna('', inplace=False).tolist())\n",
    "    #create flattened list\n",
    "    flattened_list = [item for sublist in new_list for item in sublist]\n",
    "\n",
    "    # create the vectorizer and fit\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram,\n",
    "                                 lowercase=True,\n",
    "                                 min_df=min_df)\n",
    "    vectorized_modules = vectorizer.fit_transform(flattened_list).toarray()\n",
    "\n",
    "    # some processing to make sure the vectors are the correct shape per sample\n",
    "    vectorized_tfidf = [vectorized_modules[:module_shapes[0]]]\n",
    "    for start, end in zip(module_shapes[:-1], module_shapes[1:]):\n",
    "        vectorized_tfidf.append(vectorized_modules[start:start + end])\n",
    "\n",
    "    return [torch.tensor(exe, dtype=torch.float) for exe in vectorized_tfidf]\n",
    "\n",
    "dataset = MalwareDataset(root='data', cfg=cfg)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 0.5595\n",
      "Epoch: 005, Loss: 0.2219\n",
      "Epoch: 010, Loss: 0.1165\n",
      "Epoch: 015, Loss: 0.0864\n",
      "Epoch: 020, Loss: 0.0581\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "# GNN\n",
    "model1 = torch_geometric.nn.Sequential('x, edge_index', [\n",
    "    (GCNConv(4430, 64), 'x, edge_index -> x'),\n",
    "    ReLU(inplace=True),\n",
    "    (GCNConv(64, 64), 'x, edge_index -> x'),\n",
    "    ReLU(inplace=True),\n",
    "    Linear(64, 2),\n",
    "])\n",
    "\n",
    "# MLP\n",
    "model2 = torch.nn.Sequential(\n",
    "          Linear(4430, 64),\n",
    "          ReLU(),\n",
    "          Linear(64, 64),\n",
    "          ReLU(),\n",
    "          Linear(64, 2),\n",
    "    )\n",
    "\n",
    "model = model1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(25):\n",
    "    for g in dataset[0]:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        out = model(g.x, g.edge_index)\n",
    "        loss = criterion(out, g.y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if not epoch % 5:\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
