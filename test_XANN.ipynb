{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a10555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4ab2ff-a4ae-42a6-a194-c745db810730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from models.definitions.FD import MassFD\n",
    "\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import Sequential, GCNConv, GATConv\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ae96104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XANN(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(XANN, self).__init__()\n",
    "\n",
    "        layer_dims = cfg['arch']\n",
    "        fd_layers = [] if not cfg['use_fd'] else cfg['fd_layers']\n",
    "        layer_type = cfg['layer_type']\n",
    "\n",
    "        # create the layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i, (in_d, out_d) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
    "\n",
    "            # append fd layer if needed\n",
    "            if i in fd_layers:\n",
    "                self.layers.append(MassFD(**cfg))\n",
    "\n",
    "            # append layers\n",
    "            self.layers.append(layer_type(in_d, out_d, heads=1))\n",
    "            \n",
    "            # append activation\n",
    "            self.layers.append(ReLU(inplace=True))\n",
    "        \n",
    "        if fd_layers and fd_layers[-1] == len(layer_dims) - 1:\n",
    "            self.layers.append(MassFD(**cfg))\n",
    "        self.layers.append(Linear(out_d, cfg['num_classes']))\n",
    "\n",
    "\n",
    "    def forward(self, x, y, edge_index, writer=None, epoch=None):\n",
    "\n",
    "        # forward pass\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, (GCNConv, GATConv)):\n",
    "                x = layer(x, edge_index)\n",
    "            elif isinstance(layer, (ReLU, Linear)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                fd = layer(x, y)\n",
    "                x = x + fd\n",
    "                # log value/weight\n",
    "                if writer is not None:\n",
    "                    writer.add_scalar(f'layer{i}.fd_value', fd, epoch)    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0ac31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as Data\n",
    "\n",
    "def train_loop(data, cfg):\n",
    "\n",
    "    all_metrics = {\n",
    "        \"use_fd\": [],\n",
    "        \"no_fd\": [],\n",
    "        \"use_fd_val\": [],\n",
    "        \"no_fd_val\": [],\n",
    "    }\n",
    "\n",
    "    #shutil.rmtree(f'runs/{cfg[\"dataset\"]}')\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg[\"dataset\"]}') if cfg['use_writer'] else None\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # single forward pass\n",
    "    def forward_pass(phase, epoch=0):\n",
    "\n",
    "        for graph in data:\n",
    "            # unravel data\n",
    "            X, y = graph.x.to(device), graph.y.to(device)\n",
    "            edge_index = graph.edge_index.to(device)\n",
    "\n",
    "            #generate train/val mask\n",
    "            num_nodes = X.shape[0]\n",
    "            train_nodes = int(num_nodes * 0.8)\n",
    "            rand_choice = torch.randperm(num_nodes)\n",
    "            train_mask, val_mask = rand_choice[:train_nodes], rand_choice[train_nodes:]\n",
    "                \n",
    "            # pre-amble\n",
    "            mask = train_mask if phase == \"train\" else val_mask\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # feed forward\n",
    "            logits = model(\n",
    "                x=X, y=y, edge_index=edge_index,\n",
    "                writer=writer, epoch=epoch)\n",
    "\n",
    "            # loss and backprop\n",
    "            loss = criterion(logits[mask], y[mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # predictions and accuracy\n",
    "            predictions = torch.argmax(logits[mask], dim=1)\n",
    "            accuracies = torch.sum(predictions == y[mask]).item() / len(y[mask])\n",
    "            f1 = f1_score(y[mask].cpu(), predictions.cpu(), average='macro')\n",
    "\n",
    "        # summary writing\n",
    "        if cfg['use_writer']:\n",
    "            writer.add_scalar(f'{phase}.{\"use_fd\" if use_fd else \"no_fd\"}.loss', loss, epoch)\n",
    "            for name, param in model.named_parameters():\n",
    "                if name.endswith('weight'): # only capture weights\n",
    "                    writer.add_histogram(f'{phase}.{\"use_fd\" if use_fd else \"no_fd\"}.{name}', param, epoch)\n",
    "                    writer.add_histogram(f'{phase}.{\"use_fd\" if use_fd else \"no_fd\"}.{name}.grad', param.grad, epoch)\n",
    "\n",
    "        return [loss.item(), accuracies, f1]\n",
    "\n",
    "    # start of training loop\n",
    "    for use_fd in [True, False]:\n",
    "        cfg['use_fd'] = use_fd\n",
    "        iter_metrics = []\n",
    "        for iter in range(cfg['iterations']):\n",
    "            \n",
    "            model = XANN(cfg=cfg).to(device)\n",
    "            #if iter == 0 and use_fd: print(model)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=cfg['lr'])\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=None)#torch.tensor([0.1, 2.]))\n",
    "            metrics = []\n",
    "\n",
    "            for epoch in range(cfg['epochs']):\n",
    "                # training loop\n",
    "                train_metrics = forward_pass(phase=\"train\", epoch=epoch)\n",
    "                # val loop every epoch\n",
    "                val_metrics = forward_pass(phase=\"val\", epoch=epoch)\n",
    "                metrics.append(train_metrics + val_metrics)\n",
    "            \n",
    "            iter_metrics.append(metrics)\n",
    "        all_metrics[\"use_fd\" if use_fd else \"no_fd\"] = iter_metrics\n",
    "\n",
    "    # final return as numpy arrays\n",
    "    return {k : np.array(v) for k, v in all_metrics.items()}, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "611dd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Palatino Linotype\"\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def summary_scores(all_scores, cfg, name, plot=False, save=False):\n",
    "    \n",
    "    # test for the mean difference\n",
    "    t_stat, p = ttest_ind(all_scores['no_fd'][:, -1, 3:], all_scores['use_fd'][:, -1, 3:], axis=0)\n",
    "\n",
    "    scores = \\\n",
    "            all_scores['no_fd'][:, -1, 3:].mean(0).tolist() + all_scores['no_fd'][:, -1, 3:].std(0).tolist() + \\\n",
    "            all_scores['use_fd'][:, -1, 3:].mean(0).tolist() + all_scores['use_fd'][:, -1, 3:].std(0).tolist() + \\\n",
    "            [t_stat, p]\n",
    "\n",
    "    if save:\n",
    "        with open(f'results/scores_malware.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(scores + list(cfg.values()))\n",
    "\n",
    "    if False:\n",
    "        for key in ['use_fd', 'no_fd']:\n",
    "            # prints validation metrics\n",
    "            print(key, np.around(all_scores[key][:, -1, 3:].mean(0), decimals=4).tolist())\n",
    "            print(key, np.around(all_scores[key][:, -1, 3:].std(0), decimals=4).tolist())\n",
    "            print(t_stat, p)\n",
    "\n",
    "    if plot:\n",
    "        f, axes = plt.subplots(1, 3, figsize=(14, 6))\n",
    "        #f, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "        y_labels = ['Loss', 'Accuracy', 'F1 Score']*2\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            x = np.arange(1, cfg['epochs']+1, 1)\n",
    "            y = all_scores['use_fd'].mean(0)[:, i]\n",
    "            ax.plot(x, y, '--rs', label=\"Vanilla Model\", markerfacecolor='none')\n",
    "            error = all_scores['use_fd'].std(0)[:, i]\n",
    "            ax.fill_between(x, y-error, y+error, alpha=0.2, color='r')\n",
    "\n",
    "\n",
    "            y = all_scores['no_fd'].mean(0)[:, i]\n",
    "            ax.plot(x, y, '--bo', label=\"Fractal Dimension Model\", markerfacecolor='none')\n",
    "            error = all_scores['no_fd'].std(0)[:, i]\n",
    "            ax.fill_between(x, y-error, y+error, alpha=0.2, color='b')\n",
    "            \n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Training Epoch\" if i < 3 else \"Validation Epoch\")\n",
    "            ax.set_ylabel(y_labels[i])\n",
    "            ax.set_xlim(0, cfg['epochs'])\n",
    "            if i == 0: ax.legend()\n",
    "\n",
    "        plt.savefig(f\"results/{name}.{cfg['fd_layers']}.png\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a956e156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kenneth\\AppData\\Local\\Temp\\ipykernel_5580\\3516991785.py:10: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p = ttest_ind(all_scores['no_fd'][:, -1, 3:], all_scores['use_fd'][:, -1, 3:], axis=0)\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1253: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\scipy\\stats\\_stats_py.py:1253: RuntimeWarning: invalid value encountered in multiply\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "from utils.data_loading import MalwareDataset\n",
    "from torch_geometric.datasets import Planetoid, FacebookPagePage\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "\n",
    "#names = ('file', 'reg', 'thread', 'all')\n",
    "names = ('Cora',)\n",
    "for name in names:\n",
    "    # dataset selection\n",
    "    if name in ('reg', 'file', 'thread', 'all'):\n",
    "        data = MalwareDataset(root='data', cfg={'event_type': name})[0]\n",
    "        in_dims = data[0].x.shape[1]\n",
    "        num_classes = 2\n",
    "    elif name == 'Facebook':\n",
    "        data = FacebookPagePage(root='data/', transform=RandomNodeSplit(split='train_rest', num_val=1_000, num_test=.8))[0]\n",
    "        in_dims = data.x.shape[1]\n",
    "        num_classes = data.y.max().item() + 1\n",
    "        data = (data,)\n",
    "    elif name in ('Cora', 'CiteSeer'):\n",
    "        data = Planetoid(root='data/', name=name, split='full')[0]\n",
    "        in_dims = data.x.shape[1]\n",
    "        num_classes = data.y.max().item() + 1\n",
    "        data = (data,)\n",
    "    else: \n",
    "        raise BaseException(\"Dataset not found\")\n",
    "\n",
    "    model_cfg = {\n",
    "        'arch': [in_dims, 32, 32],\n",
    "        'num_classes': num_classes,\n",
    "        'fd_layers': [1],\n",
    "        \"k\": 5,\n",
    "        \"skip\": 1,\n",
    "        \"gyration\": True,\n",
    "        \"norm\": 1.,\n",
    "        'lr': 5e-4,\n",
    "        'epochs': 30,\n",
    "        'dataset': name,\n",
    "        'use_writer': True,\n",
    "        'iterations': 1,\n",
    "        \"use_weight\": True,\n",
    "        'layer_type': GCNConv,\n",
    "        'fd_diff': False,\n",
    "        } \n",
    "\n",
    "    all_scores, writer = train_loop(data, cfg=model_cfg)\n",
    "    summary_scores(all_scores, model_cfg, name=name, plot=False, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f34adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/216 iterations\n",
      "Finished 2/216 iterations\n",
      "Finished 3/216 iterations\n",
      "Finished 4/216 iterations\n",
      "Finished 5/216 iterations\n",
      "Finished 6/216 iterations\n",
      "Finished 7/216 iterations\n",
      "Finished 8/216 iterations\n",
      "Finished 9/216 iterations\n",
      "Finished 10/216 iterations\n",
      "Finished 11/216 iterations\n",
      "Finished 12/216 iterations\n",
      "Finished 13/216 iterations\n",
      "Finished 14/216 iterations\n",
      "Finished 15/216 iterations\n",
      "Finished 16/216 iterations\n",
      "Finished 17/216 iterations\n",
      "Finished 18/216 iterations\n",
      "Finished 19/216 iterations\n",
      "Finished 20/216 iterations\n",
      "Finished 21/216 iterations\n",
      "Finished 22/216 iterations\n",
      "Finished 23/216 iterations\n",
      "Finished 24/216 iterations\n",
      "Finished 25/216 iterations\n",
      "Finished 26/216 iterations\n",
      "Finished 27/216 iterations\n",
      "Finished 28/216 iterations\n",
      "Finished 29/216 iterations\n",
      "Finished 30/216 iterations\n",
      "Finished 31/216 iterations\n",
      "Finished 32/216 iterations\n",
      "Finished 33/216 iterations\n",
      "Finished 34/216 iterations\n",
      "Finished 35/216 iterations\n",
      "Finished 36/216 iterations\n",
      "Finished 37/216 iterations\n",
      "Finished 38/216 iterations\n",
      "Finished 39/216 iterations\n",
      "Finished 40/216 iterations\n",
      "Finished 41/216 iterations\n",
      "Finished 42/216 iterations\n",
      "Finished 43/216 iterations\n",
      "Finished 44/216 iterations\n",
      "Finished 45/216 iterations\n",
      "Finished 46/216 iterations\n",
      "Finished 47/216 iterations\n",
      "Finished 48/216 iterations\n",
      "Finished 49/216 iterations\n",
      "Finished 50/216 iterations\n",
      "Finished 51/216 iterations\n",
      "Finished 52/216 iterations\n",
      "Finished 53/216 iterations\n",
      "Finished 54/216 iterations\n",
      "Finished 55/216 iterations\n",
      "Finished 56/216 iterations\n",
      "Finished 57/216 iterations\n",
      "Finished 58/216 iterations\n",
      "Finished 59/216 iterations\n",
      "Finished 60/216 iterations\n",
      "Finished 61/216 iterations\n",
      "Finished 62/216 iterations\n",
      "Finished 63/216 iterations\n",
      "Finished 64/216 iterations\n",
      "Finished 65/216 iterations\n",
      "Finished 66/216 iterations\n",
      "Finished 67/216 iterations\n",
      "Finished 68/216 iterations\n",
      "Finished 69/216 iterations\n",
      "Finished 70/216 iterations\n",
      "Finished 71/216 iterations\n",
      "Finished 72/216 iterations\n",
      "Finished 73/216 iterations\n",
      "Finished 74/216 iterations\n",
      "Finished 75/216 iterations\n",
      "Finished 76/216 iterations\n",
      "Finished 77/216 iterations\n",
      "Finished 78/216 iterations\n",
      "Finished 79/216 iterations\n",
      "Finished 80/216 iterations\n",
      "Finished 81/216 iterations\n",
      "Finished 82/216 iterations\n",
      "Finished 83/216 iterations\n",
      "Finished 84/216 iterations\n",
      "Finished 85/216 iterations\n",
      "Finished 86/216 iterations\n",
      "Finished 87/216 iterations\n",
      "Finished 88/216 iterations\n",
      "Finished 89/216 iterations\n",
      "Finished 90/216 iterations\n",
      "Finished 91/216 iterations\n",
      "Finished 92/216 iterations\n",
      "Finished 93/216 iterations\n",
      "Finished 94/216 iterations\n",
      "Finished 95/216 iterations\n",
      "Finished 96/216 iterations\n",
      "Finished 97/216 iterations\n",
      "Finished 98/216 iterations\n",
      "Finished 99/216 iterations\n",
      "Finished 100/216 iterations\n",
      "Finished 101/216 iterations\n",
      "Finished 102/216 iterations\n",
      "Finished 103/216 iterations\n",
      "Finished 104/216 iterations\n",
      "Finished 105/216 iterations\n",
      "Finished 106/216 iterations\n",
      "Finished 107/216 iterations\n",
      "Finished 108/216 iterations\n",
      "Finished 109/216 iterations\n",
      "Finished 110/216 iterations\n",
      "Finished 111/216 iterations\n",
      "Finished 112/216 iterations\n",
      "Finished 113/216 iterations\n",
      "Finished 114/216 iterations\n",
      "Finished 115/216 iterations\n",
      "Finished 116/216 iterations\n",
      "Finished 117/216 iterations\n",
      "Finished 118/216 iterations\n",
      "Finished 119/216 iterations\n",
      "Finished 120/216 iterations\n",
      "Finished 121/216 iterations\n",
      "Finished 122/216 iterations\n",
      "Finished 123/216 iterations\n",
      "Finished 124/216 iterations\n",
      "Finished 125/216 iterations\n",
      "Finished 126/216 iterations\n",
      "Finished 127/216 iterations\n",
      "Finished 128/216 iterations\n",
      "Finished 129/216 iterations\n",
      "Finished 130/216 iterations\n",
      "Finished 131/216 iterations\n",
      "Finished 132/216 iterations\n",
      "Finished 133/216 iterations\n",
      "Finished 134/216 iterations\n",
      "Finished 135/216 iterations\n",
      "Finished 136/216 iterations\n",
      "Finished 137/216 iterations\n",
      "Finished 138/216 iterations\n",
      "Finished 139/216 iterations\n",
      "Finished 140/216 iterations\n",
      "Finished 141/216 iterations\n",
      "Finished 142/216 iterations\n",
      "Finished 143/216 iterations\n",
      "Finished 144/216 iterations\n",
      "Finished 145/216 iterations\n",
      "Finished 146/216 iterations\n",
      "Finished 147/216 iterations\n",
      "Finished 148/216 iterations\n",
      "Finished 149/216 iterations\n",
      "Finished 150/216 iterations\n",
      "Finished 151/216 iterations\n",
      "Finished 152/216 iterations\n",
      "Finished 153/216 iterations\n",
      "Finished 154/216 iterations\n",
      "Finished 155/216 iterations\n",
      "Finished 156/216 iterations\n",
      "Finished 157/216 iterations\n",
      "Finished 158/216 iterations\n",
      "Finished 159/216 iterations\n",
      "Finished 160/216 iterations\n",
      "Finished 161/216 iterations\n",
      "Finished 162/216 iterations\n",
      "Finished 163/216 iterations\n",
      "Finished 164/216 iterations\n",
      "Finished 165/216 iterations\n",
      "Finished 166/216 iterations\n",
      "Finished 167/216 iterations\n",
      "Finished 168/216 iterations\n",
      "Finished 169/216 iterations\n",
      "Finished 170/216 iterations\n",
      "Finished 171/216 iterations\n",
      "Finished 172/216 iterations\n",
      "Finished 173/216 iterations\n",
      "Finished 174/216 iterations\n",
      "Finished 175/216 iterations\n",
      "Finished 176/216 iterations\n",
      "Finished 177/216 iterations\n",
      "Finished 178/216 iterations\n",
      "Finished 179/216 iterations\n",
      "Finished 180/216 iterations\n",
      "Finished 181/216 iterations\n",
      "Finished 182/216 iterations\n",
      "Finished 183/216 iterations\n",
      "Finished 184/216 iterations\n",
      "Finished 185/216 iterations\n",
      "Finished 186/216 iterations\n",
      "Finished 187/216 iterations\n",
      "Finished 188/216 iterations\n",
      "Finished 189/216 iterations\n",
      "Finished 190/216 iterations\n",
      "Finished 191/216 iterations\n",
      "Finished 192/216 iterations\n",
      "Finished 193/216 iterations\n",
      "Finished 194/216 iterations\n",
      "Finished 195/216 iterations\n",
      "Finished 196/216 iterations\n",
      "Finished 197/216 iterations\n",
      "Finished 198/216 iterations\n",
      "Finished 199/216 iterations\n",
      "Finished 200/216 iterations\n",
      "Finished 201/216 iterations\n",
      "Finished 202/216 iterations\n",
      "Finished 203/216 iterations\n",
      "Finished 204/216 iterations\n",
      "Finished 205/216 iterations\n",
      "Finished 206/216 iterations\n",
      "Finished 207/216 iterations\n",
      "Finished 208/216 iterations\n",
      "Finished 209/216 iterations\n",
      "Finished 210/216 iterations\n",
      "Finished 211/216 iterations\n",
      "Finished 212/216 iterations\n",
      "Finished 213/216 iterations\n",
      "Finished 214/216 iterations\n",
      "Finished 215/216 iterations\n",
      "Finished 216/216 iterations\n"
     ]
    }
   ],
   "source": [
    "# used for looping \n",
    "\n",
    "from itertools import product\n",
    "\n",
    "data = Planetoid(root='data/', name='Cora', split='full')[0]\n",
    "in_dims = data.x.shape[1]\n",
    "num_classes = data.y.max().item() + 1\n",
    "data = (data,)\n",
    "\n",
    "loop_cfg = {\n",
    "    'hid': [64],\n",
    "    'k': [3, 4, 5],\n",
    "    'skip': [1, 0],\n",
    "    'gyration': [True, False],\n",
    "    'norm': [1., 2., 3.],\n",
    "    'lr': [5e-3, 1e-3],\n",
    "    'layers': [[0], [1], [0, 1]]}\n",
    "    \n",
    "loop_iter = list(product(*loop_cfg.values()))\n",
    "\n",
    "for i, (hid, k, skip, gyration, norm, lr, layers) in enumerate(loop_iter):\n",
    "\n",
    "    model_cfg = {\n",
    "        'arch': [in_dims, hid, hid],\n",
    "        'num_classes': num_classes,\n",
    "        'fd_layers': layers,\n",
    "        \"k\": k,\n",
    "        \"skip\": skip,\n",
    "        \"gyration\": gyration,\n",
    "        \"norm\": norm,\n",
    "        'lr': lr,\n",
    "        'epochs': 30,\n",
    "        'dataset': 'Cora',\n",
    "        'use_writer': False,\n",
    "        'iterations': 10,\n",
    "        \"use_weight\": True,\n",
    "        'layer_type': GCNConv,\n",
    "        'fd_diff': False,\n",
    "        } \n",
    "\n",
    "    all_scores, writer = train_loop(data, cfg=model_cfg)\n",
    "    summary_scores(all_scores, model_cfg, name=name, plot=False, save=True)\n",
    "    print(f\"Finished {i+1}/{len(list(loop_iter))} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for printing graph statistics \n",
    "\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "\n",
    "def av(d):\n",
    "    return np.around(np.array(list(d.values())).mean(), decimals=4)\n",
    "def mi(d, k=0):\n",
    "    return sorted(list(d.values()))[k]\n",
    "\n",
    "names = ('all',)\n",
    "\n",
    "for name in names:\n",
    "    # dataset selection\n",
    "    if name in ('reg', 'file', 'thread', 'all'):\n",
    "        data = MalwareDataset(root='data', cfg={'event_type': name})[0]\n",
    "        in_dims = data[0].x.shape[1]\n",
    "        num_classes = 2\n",
    "\n",
    "        # compute the average metrics for all graphs in data\n",
    "        metrics = []\n",
    "        for g in data:\n",
    "            G = torch_geometric.utils.to_networkx(g)\n",
    "  \n",
    "            print(f\"{len(G.nodes)=}\")\n",
    "            print(f\"{len(G.edges)=}\")\n",
    "            print(f\"{av(nx.average_neighbor_degree(G))=}\")\n",
    "            print(f\"{mi(nx.average_neighbor_degree(G), k=0)=}\")\n",
    "            print(f\"{mi(nx.average_neighbor_degree(G), k=-1)=}\")\n",
    "            print(f\"{av(nx.average_degree_connectivity(G))=}\")\n",
    "            print(f\"{av(nx.degree_centrality(G))=}\")\n",
    "            print(f\"{nx.degree_assortativity_coefficient(G)=}\")\n",
    "            print(f\"{av(nx.clustering(G))=}\")\n",
    "            print(f\"{nx.density(G)=}\")\n",
    "            print(f\"{nx.transitivity(G)=}\")\n",
    "            print(f\"{av(nx.square_clustering(G))=}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf51d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
