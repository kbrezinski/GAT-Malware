{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4a10555",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a4ab2ff-a4ae-42a6-a194-c745db810730",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from models.definitions.FD import MassFD\n",
    "from utils.data_loading import MalwareDataset\n",
    "\n",
    "from torch.nn import Linear, ReLU\n",
    "from torch_geometric.nn import Sequential, GCNConv, GATConv\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ae96104",
   "metadata": {},
   "outputs": [],
   "source": [
    "class XANN(torch.nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(XANN, self).__init__()\n",
    "\n",
    "        layer_dims = cfg['arch']\n",
    "        fd_layers = [] if not cfg['use_fd'] else cfg['fd_layers']\n",
    "        layer_type = cfg['layer_type']\n",
    "\n",
    "        # create the layers\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i, (in_d, out_d) in enumerate(zip(layer_dims[:-1], layer_dims[1:])):\n",
    "\n",
    "            # append fd layer if needed\n",
    "            if i in fd_layers:\n",
    "                self.layers.append(MassFD(**cfg))\n",
    "\n",
    "            # append layers\n",
    "            self.layers.append(layer_type(in_d, out_d, heads=1))\n",
    "            \n",
    "            # append activation\n",
    "            self.layers.append(ReLU(inplace=True))\n",
    "        \n",
    "        if fd_layers and fd_layers[-1] == len(layer_dims) - 1:\n",
    "            self.layers.append(MassFD(**cfg))\n",
    "        self.layers.append(Linear(out_d, cfg['num_classes']))\n",
    "\n",
    "\n",
    "    def forward(self, x, y, edge_index, writer=None, epoch=None):\n",
    "\n",
    "        # forward pass\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, (GCNConv, GATConv)):\n",
    "                x = layer(x, edge_index)\n",
    "            elif isinstance(layer, (ReLU, Linear)):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                fd = layer(x, y)\n",
    "                x = x + fd\n",
    "                # log value/weight\n",
    "                if writer is not None:\n",
    "                    writer.add_scalar(f'layer{i}.fd_value', fd, epoch)    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b0ac31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric.data as Data\n",
    "\n",
    "def train_loop(data, cfg):\n",
    "\n",
    "    all_metrics = {\n",
    "        \"use_fd\": [],\n",
    "        \"no_fd\": [],\n",
    "        \"use_fd_val\": [],\n",
    "        \"no_fd_val\": [],\n",
    "    }\n",
    "\n",
    "    #shutil.rmtree(f'runs/{cfg[\"dataset\"]}')\n",
    "    writer = SummaryWriter(log_dir=f'runs/{cfg[\"dataset\"]}') if cfg['use_writer'] else None\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # single forward pass\n",
    "    def forward_pass(phase, epoch=0):\n",
    "\n",
    "        # set to proper training mode\n",
    "        if phase == \"train\":\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval() \n",
    "\n",
    "        for graph in data:\n",
    "            # unravel data\n",
    "            X, y = graph.x.to(device), graph.y.to(device)\n",
    "            edge_index = graph.edge_index.to(device)\n",
    "            mask = graph.train_mask if phase == \"train\" else graph.val_mask\n",
    "                \n",
    "            # pre-amble\n",
    "            model.train() if phase == \"train\" else model.eval()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # feed forward\n",
    "            logits = model(\n",
    "                x=X, y=y, edge_index=edge_index,\n",
    "                writer=writer, epoch=epoch)\n",
    "\n",
    "            # loss and backprop\n",
    "            loss = criterion(logits[mask], y[mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # predictions and accuracy\n",
    "            predictions = torch.argmax(logits[mask], dim=1)\n",
    "            accuracies = torch.sum(predictions == y[mask]).item() / len(y[mask])\n",
    "            f1 = f1_score(y[mask].cpu(), predictions.cpu(), average='macro')\n",
    "\n",
    "        # summary writing\n",
    "        if cfg['use_writer']:\n",
    "            writer.add_scalar(f'{phase}.{\"use_fd\" if use_fd else \"no_fd\"}.loss', loss, epoch)\n",
    "            for name, param in model.named_parameters():\n",
    "                if 'weight' in name and param.ndim == 1: # only fd capture weights\n",
    "                    writer.add_scalar(f'{phase}.{\"use_fd\" if use_fd else \"no_fd\"}.{name}', param, epoch)\n",
    "                    writer.add_scalar(f'{phase}.{\"use_fd\" if use_fd else \"no_fd\"}.{name}.grad', param.grad, epoch)\n",
    "\n",
    "        return [loss.item(), accuracies, f1]\n",
    "\n",
    "    # start of training loop\n",
    "    for use_fd in [True, False]:\n",
    "        cfg['use_fd'] = use_fd\n",
    "        iter_metrics = []\n",
    "        weight = None\n",
    "        \n",
    "        for iter in range(cfg['iterations']):\n",
    "            \n",
    "            model = XANN(cfg=cfg).to(device)\n",
    "            if cfg['dataset'] in ('reg', 'file', 'thread', 'all'):\n",
    "                weight = torch.tensor([0.1, 2.], device=device)\n",
    "                for graph in data:\n",
    "                    num_nodes = graph.x.shape[0]\n",
    "                    train_nodes = int(num_nodes * 0.9)\n",
    "                    rand_choice = torch.randperm(num_nodes)\n",
    "                    graph.train_mask = rand_choice[:train_nodes]\n",
    "                    graph.val_mask = rand_choice[train_nodes:]     \n",
    "            \n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=cfg['lr'])\n",
    "            metrics = []\n",
    "\n",
    "            for epoch in range(cfg['epochs']):\n",
    "                # training loop\n",
    "                train_metrics = forward_pass(phase=\"train\", epoch=epoch)\n",
    "                # val loop every epoch\n",
    "                val_metrics = forward_pass(phase=\"val\", epoch=epoch)\n",
    "                metrics.append(train_metrics + val_metrics)\n",
    "            \n",
    "            iter_metrics.append(metrics)\n",
    "        all_metrics[\"use_fd\" if use_fd else \"no_fd\"] = iter_metrics\n",
    "\n",
    "    # final return as numpy arrays\n",
    "    return {k : np.array(v) for k, v in all_metrics.items()}, writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "611dd710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Palatino Linotype\"\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "def summary_scores(all_scores, cfg, name, plot=False, save=False):\n",
    "    \n",
    "    # test for the mean difference\n",
    "    t_stat, p = ttest_ind(all_scores['no_fd'][:, -1, 3:], all_scores['use_fd'][:, -1, 3:], axis=0)\n",
    "\n",
    "    scores = \\\n",
    "            all_scores['no_fd'][:, -1, 3:].mean(0).tolist() + all_scores['no_fd'][:, -1, 3:].std(0).tolist() + \\\n",
    "            all_scores['use_fd'][:, -1, 3:].mean(0).tolist() + all_scores['use_fd'][:, -1, 3:].std(0).tolist() + \\\n",
    "            [t_stat, p]\n",
    "\n",
    "    if save:\n",
    "        suffix = 'malware' if cfg['dataset'] in ('reg', 'file', 'thread', 'all') else 'benchmark'\n",
    "        with open(f'results/scores_{suffix}_fd.csv', 'a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(scores + list(cfg.values()))\n",
    "\n",
    "    if False:\n",
    "        for key in ['use_fd', 'no_fd']:\n",
    "            # prints validation metrics\n",
    "            print(key, np.around(all_scores[key][:, -1, 3:].mean(0), decimals=4).tolist())\n",
    "            print(key, np.around(all_scores[key][:, -1, 3:].std(0), decimals=4).tolist())\n",
    "            print(t_stat, p)\n",
    "\n",
    "    if plot:\n",
    "        f, axes = plt.subplots(1, 3, figsize=(13, 5))\n",
    "        #f, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "        y_labels = ['Loss', 'Accuracy', 'F1 Score']*2\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            x = np.arange(1, cfg['epochs']+1, 1)\n",
    "            y = all_scores['use_fd'].mean(0)[:, i]\n",
    "            ax.plot(x, y, '--rs', label=\"Vanilla Model\", markerfacecolor='none')\n",
    "            error = all_scores['use_fd'].std(0)[:, i]\n",
    "            ax.fill_between(x, y-error, y+error, alpha=0.2, color='r')\n",
    "\n",
    "\n",
    "            y = all_scores['no_fd'].mean(0)[:, i]\n",
    "            ax.plot(x, y, '--bo', label=\"Fractal Dimension Model\", markerfacecolor='none')\n",
    "            error = all_scores['no_fd'].std(0)[:, i]\n",
    "            ax.fill_between(x, y-error, y+error, alpha=0.2, color='b')\n",
    "            \n",
    "            ax.grid()\n",
    "            ax.set_xlabel(\"Training Epoch\" if i < 3 else \"Validation Epoch\")\n",
    "            ax.set_ylabel(y_labels[i])\n",
    "            ax.set_xlim(0, cfg['epochs'])\n",
    "            if i == 0: ax.legend()\n",
    "\n",
    "        plt.savefig(f\"results/{name}.{cfg['fd_layers']}.png\", dpi=150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a956e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "from utils.data_loading import MalwareDataset\n",
    "from torch_geometric.datasets import Planetoid, FacebookPagePage\n",
    "from torch_geometric.transforms import RandomNodeSplit\n",
    "\n",
    "#names = ('file', 'reg', 'thread', 'all')\n",
    "cfg = dict(\n",
    "    event_type=('file',),\n",
    "    ngram=(1, 1),\n",
    "    features='stack',\n",
    "    )\n",
    "\n",
    "for name in cfg['event_type']:\n",
    "    # dataset selection\n",
    "    if name in ('reg', 'file', 'thread', 'all'):\n",
    "        cfg['event_type'] = name\n",
    "        data = MalwareDataset(root='data', cfg=cfg)[0]\n",
    "        in_dims = data[0].x.shape[1]\n",
    "        num_classes = 2\n",
    "    elif name == 'Facebook':\n",
    "        data = FacebookPagePage(root='data/', transform=RandomNodeSplit(split='train_rest', num_val=1_000, num_test=.8))[0]\n",
    "        in_dims = data.x.shape[1]\n",
    "        num_classes = data.y.max().item() + 1\n",
    "        data = (data,)\n",
    "    elif name in ('Cora', 'CiteSeer'):\n",
    "        data = Planetoid(root='data/', name=name, split='full')[0]\n",
    "        in_dims = data.x.shape[1]\n",
    "        num_classes = data.y.max().item() + 1\n",
    "        data = (data,)\n",
    "    else: \n",
    "        raise BaseException(\"Dataset not found\")\n",
    "\n",
    "    model_cfg = {\n",
    "        'arch': [in_dims, 64, 64],\n",
    "        'num_classes': num_classes,\n",
    "        'fd_layers': [0],\n",
    "        \"k\": 5,\n",
    "        \"skip\": 1,\n",
    "        \"gyration\": True,\n",
    "        \"norm\": 1.,\n",
    "        \"ngram\": cfg['ngram'],\n",
    "        'features': cfg['features'],\n",
    "        'lr': 5e-4,\n",
    "        'epochs': 40,\n",
    "        'dataset': name,\n",
    "        'use_writer': True,\n",
    "        'iterations': 1,\n",
    "        \"use_weight\": True,\n",
    "        'layer_type': GATConv,\n",
    "        'fd_diff': False,\n",
    "        } \n",
    "    print(model_cfg['arch'])\n",
    "\n",
    "    all_scores, writer = train_loop(data, cfg=model_cfg)\n",
    "    summary_scores(all_scores, model_cfg, name=name, plot=True, save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f34adbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .XML files...\n",
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished file dataset\n",
      "Processing .XML files...\n",
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished reg dataset\n",
      "Processing .XML files...\n",
      "Finished 1/6 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kenneth\\AppData\\Local\\Temp\\ipykernel_11068\\445880226.py:10: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p = ttest_ind(all_scores['no_fd'][:, -1, 3:], all_scores['use_fd'][:, -1, 3:], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished thread dataset\n",
      "Processing .XML files...\n",
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished all dataset\n",
      "Processing .XML files...\n",
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kenneth\\AppData\\Local\\Temp\\ipykernel_11068\\445880226.py:10: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p = ttest_ind(all_scores['no_fd'][:, -1, 3:], all_scores['use_fd'][:, -1, 3:], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished file dataset\n",
      "Processing .XML files...\n",
      "Processing ardamax.reg.csv...\n",
      "Processing asprox.reg.csv...\n",
      "Processing cerber.reg.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cryptowall.reg.csv...\n",
      "Processing dyre.reg.csv...\n",
      "Processing grayfish.reg.csv...\n",
      "Processing kelihos.reg.csv...\n",
      "Processing rustock23.reg.csv...\n",
      "Processing rustocki.reg.csv...\n",
      "Processing shamoon.reg.csv...\n",
      "Processing wannacry.reg.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished reg dataset\n",
      "Processing .XML files...\n",
      "Processing ardamax.thread.csv...\n",
      "Processing asprox.thread.csv...\n",
      "Processing cerber.thread.csv...\n",
      "Processing cryptowall.thread.csv...\n",
      "Processing dyre.thread.csv...\n",
      "Processing grayfish.thread.csv...\n",
      "Processing kelihos.thread.csv...\n",
      "Processing rustock23.thread.csv...\n",
      "Processing rustocki.thread.csv...\n",
      "Processing wannacry.thread.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished thread dataset\n",
      "Processing .XML files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ardamax.file.csv...\n",
      "Processing ardamax.reg.csv...\n",
      "Processing ardamax.thread.csv...\n",
      "Processing asprox.file.csv...\n",
      "Processing asprox.reg.csv...\n",
      "Processing asprox.thread.csv...\n",
      "Processing cerber.file.csv...\n",
      "Processing cerber.reg.csv...\n",
      "Processing cerber.thread.csv...\n",
      "Processing cryptowall.file.csv...\n",
      "Processing cryptowall.reg.csv...\n",
      "Processing cryptowall.thread.csv...\n",
      "Processing dyre.file.csv...\n",
      "Processing dyre.reg.csv...\n",
      "Processing dyre.thread.csv...\n",
      "Processing grayfish.file.csv...\n",
      "Processing grayfish.reg.csv...\n",
      "Processing grayfish.thread.csv...\n",
      "Processing kelihos.file.csv...\n",
      "Processing kelihos.reg.csv...\n",
      "Processing kelihos.thread.csv...\n",
      "Processing rustock23.file.csv...\n",
      "Processing rustock23.reg.csv...\n",
      "Processing rustock23.thread.csv...\n",
      "Processing rustocki.file.csv...\n",
      "Processing rustocki.reg.csv...\n",
      "Processing rustocki.thread.csv...\n",
      "Processing rustockj.file.csv...\n",
      "Processing shamoon.file.csv...\n",
      "Processing shamoon.reg.csv...\n",
      "Processing wannacry.file.csv...\n",
      "Processing wannacry.reg.csv...\n",
      "Processing wannacry.thread.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished all dataset\n",
      "Processing .XML files...\n",
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished file dataset\n",
      "Processing .XML files...\n",
      "Processing ardamax.reg.csv...\n",
      "Processing asprox.reg.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cerber.reg.csv...\n",
      "Processing cryptowall.reg.csv...\n",
      "Processing dyre.reg.csv...\n",
      "Processing grayfish.reg.csv...\n",
      "Processing kelihos.reg.csv...\n",
      "Processing rustock23.reg.csv...\n",
      "Processing rustocki.reg.csv...\n",
      "Processing shamoon.reg.csv...\n",
      "Processing wannacry.reg.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished reg dataset\n",
      "Processing .XML files...\n",
      "Processing ardamax.thread.csv...\n",
      "Processing asprox.thread.csv...\n",
      "Processing cerber.thread.csv...\n",
      "Processing cryptowall.thread.csv...\n",
      "Processing dyre.thread.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing grayfish.thread.csv...\n",
      "Processing kelihos.thread.csv...\n",
      "Processing rustock23.thread.csv...\n",
      "Processing rustocki.thread.csv...\n",
      "Processing wannacry.thread.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n",
      "Finished 6/6 iterations\n",
      "Finished thread dataset\n",
      "Processing .XML files...\n",
      "Processing ardamax.file.csv...\n",
      "Processing ardamax.reg.csv...\n",
      "Processing ardamax.thread.csv...\n",
      "Processing asprox.file.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing asprox.reg.csv...\n",
      "Processing asprox.thread.csv...\n",
      "Processing cerber.file.csv...\n",
      "Processing cerber.reg.csv...\n",
      "Processing cerber.thread.csv...\n",
      "Processing cryptowall.file.csv...\n",
      "Processing cryptowall.reg.csv...\n",
      "Processing cryptowall.thread.csv...\n",
      "Processing dyre.file.csv...\n",
      "Processing dyre.reg.csv...\n",
      "Processing dyre.thread.csv...\n",
      "Processing grayfish.file.csv...\n",
      "Processing grayfish.reg.csv...\n",
      "Processing grayfish.thread.csv...\n",
      "Processing kelihos.file.csv...\n",
      "Processing kelihos.reg.csv...\n",
      "Processing kelihos.thread.csv...\n",
      "Processing rustock23.file.csv...\n",
      "Processing rustock23.reg.csv...\n",
      "Processing rustock23.thread.csv...\n",
      "Processing rustocki.file.csv...\n",
      "Processing rustocki.reg.csv...\n",
      "Processing rustocki.thread.csv...\n",
      "Processing rustockj.file.csv...\n",
      "Processing shamoon.file.csv...\n",
      "Processing shamoon.reg.csv...\n",
      "Processing wannacry.file.csv...\n",
      "Processing wannacry.reg.csv...\n",
      "Processing wannacry.thread.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1/6 iterations\n",
      "Finished 2/6 iterations\n",
      "Finished 3/6 iterations\n",
      "Finished 4/6 iterations\n",
      "Finished 5/6 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kenneth\\AppData\\Local\\Temp\\ipykernel_11068\\445880226.py:10: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  t_stat, p = ttest_ind(all_scores['no_fd'][:, -1, 3:], all_scores['use_fd'][:, -1, 3:], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 6/6 iterations\n",
      "Finished all dataset\n"
     ]
    }
   ],
   "source": [
    "from itertools import product  \n",
    "# used for looping through Cora\n",
    "\n",
    "cfg = dict(\n",
    "    event_type=('file', 'reg', 'thread', 'all'),\n",
    "    ngram=((1, 1), (2, 2), (3, 3)),\n",
    "    features='stack',\n",
    "    )\n",
    "\n",
    "for ngram in cfg['ngram']:\n",
    "    for name in cfg['event_type']:\n",
    "\n",
    "        # dataset selection\n",
    "        if name in ('reg', 'file', 'thread', 'all'):\n",
    "            data_cfg = dict(event_type=name, ngram=ngram, features='stack')\n",
    "            data = MalwareDataset(root='data', cfg=data_cfg)[0]\n",
    "            in_dims = data[0].x.shape[1]\n",
    "            num_classes = 2\n",
    "        else:\n",
    "            data = Planetoid(root='data/', name=name, split='full')[0]\n",
    "            in_dims = data.x.shape[1]\n",
    "            num_classes = data.y.max().item() + 1\n",
    "            data = (data,)\n",
    "\n",
    "        loop_cfg = {\n",
    "            'hid': [64],\n",
    "            'k': [4],\n",
    "            'skip': [1],\n",
    "            'gyration': [True, False],\n",
    "            'norm': [1.],\n",
    "            'lr': [5e-3],\n",
    "            'layers': [[0], [1], [0, 1]],}\n",
    "   \n",
    "        loop_iter = list(product(*loop_cfg.values()))\n",
    "        for i, (hid, k, skip, gyration, norm, lr, layers) in enumerate(loop_iter):\n",
    "\n",
    "            model_cfg = {\n",
    "                'arch': [in_dims, hid, hid],\n",
    "                'num_classes': num_classes,\n",
    "                'fd_layers': layers,\n",
    "                \"k\": k,\n",
    "                \"skip\": skip,\n",
    "                \"gyration\": gyration,\n",
    "                \"norm\": norm,\n",
    "                'lr': lr,\n",
    "                'ngram': ngram,\n",
    "                'epochs': 40,\n",
    "                'dataset': name,\n",
    "                'use_writer': False,\n",
    "                'iterations': 5,\n",
    "                \"use_weight\": True,\n",
    "                'layer_type': GATConv,\n",
    "                'fd_diff': False,\n",
    "                } \n",
    "\n",
    "            all_scores, writer = train_loop(data, cfg=model_cfg)\n",
    "            summary_scores(all_scores, model_cfg, name=name, plot=False, save=True)\n",
    "            print(f\"Finished {i+1}/{len(list(loop_iter))} iterations\")\n",
    "        print(f\"Finished {name} dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0715eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this for printing graph statistics \n",
    "\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "\n",
    "def av(d):\n",
    "    return np.around(np.array(list(d.values())).mean(), decimals=4)\n",
    "def mi(d, k=0):\n",
    "    return sorted(list(d.values()))[k]\n",
    "\n",
    "names = ('all',)\n",
    "\n",
    "for name in names:\n",
    "    # dataset selection\n",
    "    if name in ('reg', 'file', 'thread', 'all'):\n",
    "        data = MalwareDataset(root='data', cfg={'event_type': name})[0]\n",
    "        in_dims = data[0].x.shape[1]\n",
    "        num_classes = 2\n",
    "\n",
    "        # compute the average metrics for all graphs in data\n",
    "        metrics = []\n",
    "        for g in data:\n",
    "            G = torch_geometric.utils.to_networkx(g)\n",
    "  \n",
    "            print(f\"{len(G.nodes)=}\")\n",
    "            print(f\"{len(G.edges)=}\")\n",
    "            print(f\"{av(nx.average_neighbor_degree(G))=}\")\n",
    "            print(f\"{mi(nx.average_neighbor_degree(G), k=0)=}\")\n",
    "            print(f\"{mi(nx.average_neighbor_degree(G), k=-1)=}\")\n",
    "            print(f\"{av(nx.average_degree_connectivity(G))=}\")\n",
    "            print(f\"{av(nx.degree_centrality(G))=}\")\n",
    "            print(f\"{nx.degree_assortativity_coefficient(G)=}\")\n",
    "            print(f\"{av(nx.clustering(G))=}\")\n",
    "            print(f\"{nx.density(G)=}\")\n",
    "            print(f\"{nx.transitivity(G)=}\")\n",
    "            print(f\"{av(nx.square_clustering(G))=}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf51d30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
