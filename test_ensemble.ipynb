{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "from utils.data_loading import MalwareDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .XML files...\n",
      "Processing .XML files...\n",
      "Processing .XML files...\n",
      "Finished saving ardamax.reg.csv to data\\raw\n",
      "Finished saving asprox.reg.csv to data\\raw\n",
      "Finished saving cerber.reg.csv to data\\raw\n",
      "Finished saving cryptowall.reg.csv to data\\raw\n",
      "Finished saving dyre.reg.csv to data\\raw\n",
      "Finished saving grayfish.reg.csv to data\\raw\n",
      "Finished saving kelihos.reg.csv to data\\raw\n",
      "Finished saving rustock23.reg.csv to data\\raw\n",
      "Finished saving rustocki.reg.csv to data\\raw\n",
      "Finished saving shamoon.reg.csv to data\\raw\n",
      "Finished saving wannacry.reg.csv to data\\raw\n",
      "Processing ardamax.reg.csv...\n",
      "Processing asprox.reg.csv...\n",
      "Processing cerber.reg.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing cryptowall.reg.csv...\n",
      "Processing dyre.reg.csv...\n",
      "Processing grayfish.reg.csv...\n",
      "Processing kelihos.reg.csv...\n",
      "Processing rustock23.reg.csv...\n",
      "Processing rustocki.reg.csv...\n",
      "Processing shamoon.reg.csv...\n",
      "Processing wannacry.reg.csv...\n",
      "[[Data(edge_index=[2, 113], y=[40], x=[40, 2240]), Data(edge_index=[2, 116], y=[41], x=[41, 2240]), Data(edge_index=[2, 137], y=[48], x=[48, 2240]), Data(edge_index=[2, 125], y=[44], x=[44, 2240]), Data(edge_index=[2, 128], y=[45], x=[45, 2240]), Data(edge_index=[2, 119], y=[42], x=[42, 2240]), Data(edge_index=[2, 116], y=[41], x=[41, 2240]), Data(edge_index=[2, 122], y=[43], x=[43, 2240]), Data(edge_index=[2, 116], y=[41], x=[41, 2240]), Data(edge_index=[2, 407], y=[138], x=[138, 2240]), Data(edge_index=[2, 140], y=[49], x=[49, 2240]), Data(edge_index=[2, 131], y=[46], x=[46, 2240])], [Data(edge_index=[2, 113], y=[40], x=[40, 678]), Data(edge_index=[2, 116], y=[41], x=[41, 678]), Data(edge_index=[2, 137], y=[48], x=[48, 678]), Data(edge_index=[2, 125], y=[44], x=[44, 678]), Data(edge_index=[2, 128], y=[45], x=[45, 678]), Data(edge_index=[2, 119], y=[42], x=[42, 678]), Data(edge_index=[2, 116], y=[41], x=[41, 678]), Data(edge_index=[2, 122], y=[43], x=[43, 678]), Data(edge_index=[2, 116], y=[41], x=[41, 678]), Data(edge_index=[2, 131], y=[46], x=[46, 678])], [Data(edge_index=[2, 113], y=[40], x=[40, 2211]), Data(edge_index=[2, 116], y=[41], x=[41, 2211]), Data(edge_index=[2, 137], y=[48], x=[48, 2211]), Data(edge_index=[2, 125], y=[44], x=[44, 2211]), Data(edge_index=[2, 128], y=[45], x=[45, 2211]), Data(edge_index=[2, 119], y=[42], x=[42, 2211]), Data(edge_index=[2, 116], y=[41], x=[41, 2211]), Data(edge_index=[2, 122], y=[43], x=[43, 2211]), Data(edge_index=[2, 116], y=[41], x=[41, 2211]), Data(edge_index=[2, 140], y=[49], x=[49, 2211]), Data(edge_index=[2, 131], y=[46], x=[46, 2211])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "cfg = dict(\n",
    "    event_type=('file','thread','reg'),\n",
    "    ngram=(1, 1),\n",
    "    features='stack',\n",
    "    )\n",
    "\n",
    "datasets = []\n",
    "for name in cfg['event_type']:\n",
    "   # dataset selection \n",
    "    cfg['event_type'] = name\n",
    "    datasets.append(MalwareDataset(root='data', cfg=cfg)[0])\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GCNConv, GATConv, Sequential\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.net = Sequential(\n",
    "            \"x, edge_index\",[\n",
    "            (GATConv(-1, embed_dim), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (GATConv(embed_dim, embed_dim), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.net(x, edge_index)\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, hid_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.LazyLinear(hid_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EnsembleGNN(nn.Module):\n",
    "    def __init__(self, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.net1 = GNN(embed_dim)\n",
    "        self.net2 = GNN(embed_dim)\n",
    "        self.net3 = GNN(embed_dim)\n",
    "        self.ff = Feedforward(hid_dim=embed_dim*2)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        d1, d2, d3 = data\n",
    "        x1 = self.net1(d1.x, d1.edge_index)\n",
    "        x2 = self.net2(d2.x, d2.edge_index)\n",
    "        x3 = self.net3(d3.x, d3.edge_index)\n",
    "        # (n, dims) -> (n * 3, dims)\n",
    "        out = torch.cat([x1, x2, x3], dim=0)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.ff(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class StackedGNN(nn.Module):\n",
    "    def __init__(self, embed_dim=32):\n",
    "        super().__init__()\n",
    "        self.net1 = GNN(embed_dim)\n",
    "        self.ff = Feedforward(hid_dim=embed_dim*2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = torch.cat([d.x for d in data], dim=1)\n",
    "        print(x)\n",
    "        x1 = self.net1(d1.x, d1.edge_index)\n",
    "        x2 = self.net2(d2.x, d2.edge_index)\n",
    "        x3 = self.net3(d3.x, d3.edge_index)\n",
    "        # (n, dims) -> (n * 3, dims)\n",
    "        out = torch.cat([x1, x2, x3], dim=0)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'StackedGNN' object has no attribute 'net2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdatasets):\n\u001b[0;32m     16\u001b[0m     \n\u001b[0;32m     17\u001b[0m     \u001b[39m# zero grad and train\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m     out \u001b[39m=\u001b[39m model(data)\n\u001b[0;32m     21\u001b[0m     \u001b[39m# compute loss and accuracy\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([d\u001b[39m.\u001b[39my \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data], axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 58\u001b[0m, in \u001b[0;36mStackedGNN.forward\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     56\u001b[0m d1, d2, d3 \u001b[39m=\u001b[39m data\n\u001b[0;32m     57\u001b[0m x1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet1(d1\u001b[39m.\u001b[39mx, d1\u001b[39m.\u001b[39medge_index)\n\u001b[1;32m---> 58\u001b[0m x2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet2(d2\u001b[39m.\u001b[39mx, d2\u001b[39m.\u001b[39medge_index)\n\u001b[0;32m     59\u001b[0m x3 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet3(d3\u001b[39m.\u001b[39mx, d3\u001b[39m.\u001b[39medge_index)\n\u001b[0;32m     60\u001b[0m \u001b[39m# (n, dims) -> (n * 3, dims)\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'StackedGNN' object has no attribute 'net2'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = StackedGNN(embed_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "criterion = nn.CrossEntropyLoss(weight=None)#torch.tensor([1., 2.]))\n",
    "\n",
    "for epoch in range(500):\n",
    "    \n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "    cum_f1 = 0\n",
    "    num_graphs = len(datasets[0])\n",
    "\n",
    "    # loop through graph models by variant\n",
    "    for data in zip(*datasets):\n",
    "        \n",
    "        # zero grad and train\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data)\n",
    "\n",
    "        # compute loss and accuracy\n",
    "        labels = torch.cat([d.y for d in data], axis=0)\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "        # predictions and accuracy\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "        accuracies = torch.sum(predictions == labels).item() / len(labels)\n",
    "\n",
    "        cum_acc += accuracies\n",
    "        cum_f1 += f1_score(labels.cpu(), predictions.cpu(), average='macro')\n",
    "\n",
    "    #f1 = f1_score(y[mask].cpu(), predictions.cpu(), average='macro')\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"{epoch=}, {cum_loss=:.4f}\")\n",
    "        print(f\"cum_f1={cum_f1 / num_graphs:.4f}, cum_acc={cum_acc / num_graphs:.4f}\")\n",
    "        print(\"#\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
