{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "from gatmalware.data import MalwareDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing .XML files...\n"
     ]
    }
   ],
   "source": [
    "cfg = dict(\n",
    "    event_type=('file',),\n",
    "    ngram=(1, 1),\n",
    "    features='stack',\n",
    "    )\n",
    "\n",
    "datasets = []\n",
    "for name in cfg['event_type']:\n",
    "   # dataset selection \n",
    "    cfg['event_type'] = name\n",
    "    datasets.append(MalwareDataset(root='data', cfg=cfg)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import GATConv, Sequential\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = Sequential(\n",
    "            \"x, edge_index\",[\n",
    "            (GATConv(-1, embed_dim), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "            (GATConv(embed_dim, embed_dim), 'x, edge_index -> x'),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "    def forward(self, x, edge_index):\n",
    "        return self.net(x, edge_index)\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, hid_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(hid_dim, 2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class EnsembleGNN(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.net1 = GNN(embed_dim)\n",
    "        self.net2 = GNN(embed_dim)\n",
    "        self.net3 = GNN(embed_dim)\n",
    "        self.ff = Feedforward(hid_dim=embed_dim*3)\n",
    "        self.layer_norm = nn.LayerNorm(embed_dim, elementwise_affine=False)\n",
    "\n",
    "    def forward(self, data):\n",
    "        d1, d2, d3 = data\n",
    "        x1 = self.net1(d1.x, d1.edge_index)\n",
    "        x2 = self.net2(d2.x, d2.edge_index)\n",
    "        x3 = self.net3(d3.x, d3.edge_index)\n",
    "        # (n, dims) -> (n * 3, dims)\n",
    "        out = torch.cat([x1, x2, x3], dim=0)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.ff(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class StackedGNN(nn.Module):\n",
    "    def __init__(self, embed_dim=128):\n",
    "        super().__init__()\n",
    "        self.net1 = GNN(embed_dim)\n",
    "        self.ff = Feedforward(hid_dim=embed_dim)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = torch.cat([d.x for d in data], dim=1)\n",
    "        print(x)\n",
    "        x1 = self.net1(d1.x, d1.edge_index)\n",
    "        x2 = self.net2(d2.x, d2.edge_index)\n",
    "        x3 = self.net3(d3.x, d3.edge_index)\n",
    "        # (n, dims) -> (n * 3, dims)\n",
    "        out = torch.cat([x1, x2, x3], dim=0)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.ff(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "net1.net.module_0.att_src torch.Size([1, 1, 128])\n",
      "net1.net.module_0.att_dst torch.Size([1, 1, 128])\n",
      "net1.net.module_0.bias torch.Size([128])\n",
      "net1.net.module_0.lin_src.weight torch.Size([0])\n",
      "net1.net.module_2.att_src torch.Size([1, 1, 128])\n",
      "net1.net.module_2.att_dst torch.Size([1, 1, 128])\n",
      "net1.net.module_2.bias torch.Size([128])\n",
      "net1.net.module_2.lin_src.weight torch.Size([128, 128])\n",
      "ff.net.0.weight torch.Size([2, 128])\n",
      "ff.net.0.bias torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "model = StackedGNN(embed_dim=128).to('cuda')\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, cum_loss=8.2853\n",
      "cum_f1=0.4499, cum_acc=0.4658\n",
      "###################################\n",
      "epoch=100, cum_loss=1.7891\n",
      "cum_f1=0.9445, cum_acc=0.8039\n",
      "###################################\n",
      "epoch=200, cum_loss=1.5743\n",
      "cum_f1=0.9519, cum_acc=0.8265\n",
      "###################################\n",
      "epoch=300, cum_loss=1.3155\n",
      "cum_f1=0.9573, cum_acc=0.8622\n",
      "###################################\n",
      "epoch=400, cum_loss=1.2983\n",
      "cum_f1=0.9541, cum_acc=0.8636\n",
      "###################################\n",
      "epoch=500, cum_loss=1.3768\n",
      "cum_f1=0.9527, cum_acc=0.8609\n",
      "###################################\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, balanced_accuracy_score\n",
    "\n",
    "model = GNN(embed_dim=64).to('cuda')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=2e-3)\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.tensor([1., 2.]).to('cuda'))\n",
    "\n",
    "for epoch in range(501):\n",
    "    \n",
    "    cum_loss = 0\n",
    "    cum_acc = 0\n",
    "    cum_f1 = 0\n",
    "    num_graphs = len(datasets[0])\n",
    "\n",
    "    # loop through graph models by variant\n",
    "    for data in datasets[0]:\n",
    "\n",
    "        # unravel the data\n",
    "        labels = data.y.to('cuda')\n",
    "        x = data.x.to('cuda')\n",
    "        edge_index = data.edge_index.to('cuda')\n",
    "\n",
    "        # zero grad and train\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x, edge_index)\n",
    "        \n",
    "        # compute loss and accuracy\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        cum_loss += loss.item()\n",
    "\n",
    "        # predictions and accuracy\n",
    "        predictions = torch.argmax(out, dim=1)\n",
    "        accuracies = balanced_accuracy_score(labels.cpu(), predictions.cpu())\n",
    "\n",
    "        cum_acc += accuracies\n",
    "        *_, f1_score, _ = precision_recall_fscore_support(labels.cpu(), predictions.cpu(), average='weighted', zero_division=0)\n",
    "        cum_f1 += f1_score\n",
    "\n",
    "    if not epoch % 100:\n",
    "        print(f\"{epoch=}, {cum_loss=:.4f}\")\n",
    "        print(f\"cum_f1={cum_f1 / num_graphs:.4f}, cum_acc={cum_acc / num_graphs:.4f}\")\n",
    "        print(\"#\"*35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"models/binaries/stack.(1, 1).file.pkl\",'rb') as f:\n",
    "    tfidf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 18\u001b[0m\n\u001b[0;32m      5\u001b[0m explainer \u001b[39m=\u001b[39m Explainer(\n\u001b[0;32m      6\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m      7\u001b[0m     algorithm\u001b[39m=\u001b[39mGNNExplainer(epochs\u001b[39m=\u001b[39m\u001b[39m200\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m     ),\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m node_index \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[1;32m---> 18\u001b[0m explanation \u001b[39m=\u001b[39m explainer(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index, index\u001b[39m=\u001b[39;49mnode_index)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\explain\\explainer.py:192\u001b[0m, in \u001b[0;36mExplainer.__call__\u001b[1;34m(self, x, edge_index, target, index, **kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[39mif\u001b[39;00m target \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    189\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    190\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m'\u001b[39m\u001b[39m should not be provided for the explanation \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtype \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexplanation_type\u001b[39m.\u001b[39mvalue\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 192\u001b[0m     prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_prediction(x, edge_index, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    193\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_target(prediction)\n\u001b[0;32m    195\u001b[0m training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtraining\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\explain\\explainer.py:115\u001b[0m, in \u001b[0;36mExplainer.get_prediction\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[0;32m    114\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m--> 115\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain(training)\n\u001b[0;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[4], line 17\u001b[0m, in \u001b[0;36mGNN.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index):\n\u001b[1;32m---> 17\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet(x, edge_index)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\Kenneth_pyg\\tmpcrv_p_wq.py:18\u001b[0m, in \u001b[0;36mSequential_3752ca.forward\u001b[1;34m(self, x, edge_index)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index):\n\u001b[0;32m     17\u001b[0m     \u001b[39m\"\"\"\"\"\"\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodule_0(x, edge_index)\n\u001b[0;32m     19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_1(x)\n\u001b[0;32m     20\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodule_2(x, edge_index)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:213\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, Tensor):\n\u001b[0;32m    212\u001b[0m     \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mdim() \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mStatic graphs not supported in \u001b[39m\u001b[39m'\u001b[39m\u001b[39mGATConv\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 213\u001b[0m     x_src \u001b[39m=\u001b[39m x_dst \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin_src(x)\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, H, C)\n\u001b[0;32m    214\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# Tuple of source and target node features:\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     x_src, x_dst \u001b[39m=\u001b[39m x\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:132\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    128\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "\n",
    "data = datasets[0][0]\n",
    "\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type='attributes',\n",
    "    edge_mask_type='object',\n",
    "    model_config=dict(\n",
    "        mode='multiclass_classification',\n",
    "        task_level='node',\n",
    "        return_type='log_probs',\n",
    "    ),\n",
    ")\n",
    "node_index = 10\n",
    "explanation = explainer(data.x, data.edge_index, index=node_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
