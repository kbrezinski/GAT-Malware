{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generates the torch.data version of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.config import *\n",
    "\n",
    "from collections import defaultdict\n",
    "from xml.etree import cElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import InMemoryDataset, Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving file.anki.csv to d:\\Code\\Papers\\GAT\\utils\\..\\data\\raw\n",
      "Finished saving file.slack.csv to d:\\Code\\Papers\\GAT\\utils\\..\\data\\raw\n"
     ]
    }
   ],
   "source": [
    "def process_xml(base_path):\n",
    "\n",
    "    # append raw path to base_dir\n",
    "    XML_PATH = os.path.join(base_path, 'xml')\n",
    "    event_type = cfg['event_type']\n",
    "    \n",
    "    # loop through .XML files\n",
    "    for filename in os.listdir(XML_PATH):\n",
    "        \n",
    "        # loop through relevent event type xml files\n",
    "        if filename.endswith(f\"{event_type}.XML\"):\n",
    "            \n",
    "            # get the full path and root of xml file\n",
    "            path = os.path.join(base_path, 'xml', filename)\n",
    "            root = ET.parse(path).getroot()\n",
    "            \n",
    "            # get the target name of the malware executable\n",
    "            target_name = filename.split(\".\")[1].lower()\n",
    "            \n",
    "            attr_dict = defaultdict(list)\n",
    "\n",
    "            # root[0] is process, root[1] is event\n",
    "            for child in root[0]:\n",
    "                for event in child:\n",
    "                    tag, text = event.tag, event.text\n",
    "\n",
    "                    # get the module list\n",
    "                    if tag == 'modulelist':\n",
    "\n",
    "                        # build up the module list\n",
    "                        empty_stack = []\n",
    "                        # the path tag is the 4th element in the module list\n",
    "                        for module in event:\n",
    "                            empty_stack.append(module[3].text.split('\\\\')[-1])\n",
    "\n",
    "                        # create the module feature\n",
    "                        attr_dict['modules'].append(' '.join(empty_stack))\n",
    "\n",
    "                    # capture the other keys\n",
    "                    else:\n",
    "                        attr_dict[tag].append(text)\n",
    "                      \n",
    "            # create save dir and save it\n",
    "            save_dir = os.path.join(base_path, 'raw')\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "            save_name = f'{event_type}.{target_name}.csv'\n",
    "            df = pd.DataFrame(attr_dict).to_csv(\n",
    "                os.path.join(save_dir, save_name), index=False)\n",
    "            print(f\"Finished saving {save_name} to {save_dir}\")\n",
    "\n",
    "cfg = dict(\n",
    "    event_type = 'file',\n",
    ")    \n",
    "process_xml(DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing file.anki.csv\n",
      "Finished processing file.slack.csv\n",
      "Unit test passed\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def assign_labels(df, file_name):\n",
    "\n",
    "    prev_len = -1\n",
    "    target_name = file_name.split(\".\")[-2]\n",
    "\n",
    "    # get the malicious pids of the parent process\n",
    "    df['ProcessName'] = df['ProcessName'].apply(lambda x: x.lower())\n",
    "    unique_pids = df[df['ProcessName'] == target_name + '.exe']['ProcessId'].unique()\n",
    "\n",
    "    assert len(unique_pids) > 0, f\"Target name not found in {file_name}\"\n",
    "    mal_pids = set(unique_pids)\n",
    "\n",
    "    # stops when the length of pids stops growing\n",
    "    while prev_len != len(mal_pids):\n",
    "        new_rows = df[df['ParentProcessId'].isin(mal_pids)]\n",
    "        new_pids = set(new_rows['ProcessId'].unique())\n",
    "\n",
    "        # updates the lengths and the malicious pids\n",
    "        prev_len = len(mal_pids)\n",
    "        mal_pids.update(new_pids)\n",
    "\n",
    "    df['label'] = df['ProcessId'].apply(lambda x: 1 if x in mal_pids else 0)\n",
    "    \n",
    "    return torch.tensor(df['label'].values, dtype=torch.float)\n",
    "\n",
    "\n",
    "def get_feature_vectors(df, ngram=(2, 2), min_df=1):\n",
    "    \n",
    "    # exposed the args for future tuning\n",
    "    # setting min_df to 1 removes the imports from the filename\n",
    "    vectorizer = TfidfVectorizer(ngram_range=ngram,\n",
    "                                 lowercase=True,\n",
    "                                 min_df=min_df)\n",
    "    # fill nans with space\n",
    "    df['modules'].fillna('', inplace=True)\n",
    "    \n",
    "    return torch.tensor(vectorizer.fit_transform(df['modules']).toarray(), dtype=torch.float)\n",
    "\n",
    "\n",
    "def get_edge_list(graph_nodes, add_self_edges=True):\n",
    "    \n",
    "    ## Create the adjacency list using nx\n",
    "    G = nx.from_pandas_edgelist(graph_nodes, 'ProcessId', 'ParentProcessId')\n",
    "    \n",
    "    ## Create some empty lists\n",
    "    trg, src = [], []\n",
    "    node_list = []\n",
    "    \n",
    "    ## Loop through the adjacency list\n",
    "    for n, nbrdict in G.adjacency():\n",
    "        for target in nbrdict.keys():\n",
    "            src.append(n)\n",
    "            trg.append(target)\n",
    "            \n",
    "        ## Append the PID and the target\n",
    "        node_list.append(n)\n",
    "\n",
    "    ## Add self-edges if set to true\n",
    "    if add_self_edges:\n",
    "        src.extend(node_list)\n",
    "        trg.extend(node_list)\n",
    "        \n",
    "    ## Convert to numpy arrays [2, E]\n",
    "    edge_index = np.row_stack((src, trg))\n",
    "    \n",
    "    return torch.tensor(edge_index, dtype=torch.int32)\n",
    "\n",
    "\n",
    "def unit_test():\n",
    "    ## main\n",
    "    attr_base = os.path.join(DATA_DIR, 'raw')\n",
    "\n",
    "    for filename in os.listdir(attr_base):\n",
    "        graph_nodes = pd.read_csv(attr_base + '\\\\' + filename)\n",
    "        \n",
    "        ## Assign labels and return array\n",
    "        target_list = assign_labels(graph_nodes, filename)\n",
    "        \n",
    "        ## Get the processed feature list\n",
    "        feature_list = get_feature_vectors(graph_nodes, ngram=(2, 2), min_df=2)\n",
    "        \n",
    "        ## Get the edge list\n",
    "        edge_list = get_edge_list(graph_nodes)\n",
    "        \n",
    "        # Sanity check for dimensions of feature and target\n",
    "        assert feature_list.shape[0] == len(target_list),\\\n",
    "                    f\"Shape mismatch, feature length: {feature_list.shape[0]} \\\n",
    "                                    target length: {len(target_list)}\"\n",
    "            \n",
    "        print(f\"Finished processing {filename}\")\n",
    "    print(\"Unit test passed\")\n",
    "    \n",
    "unit_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download Triggered\n",
      "Download...\n",
      "Dimensions for file.anki.csv:\n",
      "\t Data(x=[263, 1322], edge_index=[2, 798], y=[263])\n",
      "Dimensions for file.slack.csv:\n",
      "\t Data(x=[243, 1164], edge_index=[2, 737], y=[243])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 1322 but got size 1164 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32md:\\Code\\Papers\\GAT\\test_graph_gen.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 71>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m         torch\u001b[39m.\u001b[39msave((data, slices), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m cfg \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     event_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m     generate_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m dataset \u001b[39m=\u001b[39m MalwareDataset(root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m, cfg\u001b[39m=\u001b[39;49mcfg)\n",
      "\u001b[1;32md:\\Code\\Papers\\GAT\\test_graph_gen.ipynb Cell 7\u001b[0m in \u001b[0;36mMalwareDataset.__init__\u001b[1;34m(self, root, cfg, transform, pre_transform)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroot \u001b[39m=\u001b[39m root\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevent_type \u001b[39m=\u001b[39m cfg[\u001b[39m'\u001b[39m\u001b[39mevent_type\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39msuper\u001b[39;49m(MalwareDataset, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform)\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:50\u001b[0m, in \u001b[0;36mInMemoryDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, root: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m              transform: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m              pre_transform: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m              pre_filter: Optional[Callable] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 50\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(root, transform, pre_transform, pre_filter)\n\u001b[0;32m     51\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mslices \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\data\\dataset.py:87\u001b[0m, in \u001b[0;36mDataset.__init__\u001b[1;34m(self, root, transform, pre_transform, pre_filter)\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_download()\n\u001b[0;32m     86\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mDataset\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m---> 87\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process()\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\data\\dataset.py:170\u001b[0m, in \u001b[0;36mDataset._process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mProcessing...\u001b[39m\u001b[39m'\u001b[39m, file\u001b[39m=\u001b[39msys\u001b[39m.\u001b[39mstderr)\n\u001b[0;32m    169\u001b[0m makedirs(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir)\n\u001b[1;32m--> 170\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess()\n\u001b[0;32m    172\u001b[0m path \u001b[39m=\u001b[39m osp\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_dir, \u001b[39m'\u001b[39m\u001b[39mpre_transform.pt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    173\u001b[0m torch\u001b[39m.\u001b[39msave(_repr(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_transform), path)\n",
      "\u001b[1;32md:\\Code\\Papers\\GAT\\test_graph_gen.ipynb Cell 7\u001b[0m in \u001b[0;36mMalwareDataset.process\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m     exe_graphs\u001b[39m.\u001b[39mappend(data)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m \u001b[39m# colalte the list and save the data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m data, slices \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate(exe_graphs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Code/Papers/GAT/test_graph_gen.ipynb#W6sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m torch\u001b[39m.\u001b[39msave((data, slices), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessed_paths[\u001b[39m0\u001b[39m])\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\data\\in_memory_dataset.py:105\u001b[0m, in \u001b[0;36mInMemoryDataset.collate\u001b[1;34m(data_list)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data_list) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    103\u001b[0m     \u001b[39mreturn\u001b[39;00m data_list[\u001b[39m0\u001b[39m], \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m data, slices, _ \u001b[39m=\u001b[39m collate(\n\u001b[0;32m    106\u001b[0m     data_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49m\u001b[39m__class__\u001b[39;49m,\n\u001b[0;32m    107\u001b[0m     data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[0;32m    108\u001b[0m     increment\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    109\u001b[0m     add_batch\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    110\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mreturn\u001b[39;00m data, slices\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\data\\collate.py:84\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[39m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m value, slices, incs \u001b[39m=\u001b[39m _collate(attr, values, data_list, stores,\n\u001b[0;32m     85\u001b[0m                                increment)\n\u001b[0;32m     87\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor) \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mis_cuda:\n\u001b[0;32m     88\u001b[0m     device \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[1;32me:\\anaconda3\\envs\\gat\\lib\\site-packages\\torch_geometric\\data\\collate.py:155\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m         out \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 155\u001b[0m     value \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(values, dim\u001b[39m=\u001b[39;49mcat_dim \u001b[39mor\u001b[39;49;00m \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n\u001b[0;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m value, slices, incs\n\u001b[0;32m    158\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, SparseTensor) \u001b[39mand\u001b[39;00m increment:\n\u001b[0;32m    159\u001b[0m     \u001b[39m# Concatenate a list of `SparseTensor` along the `cat_dim`.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[39m# NOTE: `cat_dim` may return a tuple to allow for diagonal stacking.\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 1322 but got size 1164 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "class MalwareDataset(InMemoryDataset):\n",
    "    \n",
    "    def __init__(self, root, cfg, transform=None, pre_transform=None):\n",
    "        \"\"\"\n",
    "        root = where the dataset should be stored\n",
    "        \"\"\"\n",
    "        self.root = root\n",
    "        self.event_type = cfg['event_type']\n",
    "        \n",
    "        super(MalwareDataset, self).__init__(root, transform, pre_transform)\n",
    "        \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        # if this file exists in /raw, the download is not triggered\n",
    "        print(\"Download Triggered\")\n",
    "        return 'not_implemented.csv'\n",
    "    \n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        # if this file exists in /processed, process is not triggered\n",
    "        return [self.event_type + '.pt'] if not cfg['generate_graph'] else 'foo.pt'\n",
    "    \n",
    "    # ignore the download helper fn\n",
    "    def download(self):\n",
    "        print(\"Download...\")\n",
    "        return \n",
    "\n",
    "    def len(self):\n",
    "        return 1\n",
    "\n",
    "    # main process \n",
    "    def process(self):\n",
    "        \n",
    "        # parent list for all the executable graphs\n",
    "        exe_graphs = []\n",
    "\n",
    "        # loop through all the files and read csv\n",
    "        for filename in os.listdir(self.raw_dir):\n",
    "            graph_nodes = pd.read_csv(self.raw_dir + '\\\\' + filename)\n",
    "\n",
    "            # Assign labels and return array\n",
    "            target_list = assign_labels(graph_nodes, filename)\n",
    "\n",
    "            # Get the processed feature list\n",
    "            feature_list = get_feature_vectors(graph_nodes, ngram=(2, 2), min_df=2)\n",
    "\n",
    "            # Get the edge list\n",
    "            edge_list = get_edge_list(graph_nodes)\n",
    "\n",
    "            # Create the data object\n",
    "            data = Data(\n",
    "                x = feature_list,\n",
    "                edge_index = edge_list,\n",
    "                y = target_list\n",
    "            )\n",
    "\n",
    "            # Sanity check for dimensions of feature and target\n",
    "            print(f\"Dimensions for {filename}:\\n\\t {data}\")\n",
    "            exe_graphs.append(data)\n",
    "        \n",
    "        # collate the list and save the data\n",
    "        data, slices = self.collate(exe_graphs)\n",
    "        torch.save((data, slices), self.processed_paths[0])\n",
    "        \n",
    "        \n",
    "cfg = dict(\n",
    "    event_type='file',\n",
    "    generate_graph=True\n",
    ")\n",
    "\n",
    "dataset = MalwareDataset(root='data', cfg=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty master dict for all samples and attributes\n",
    "master_repo = defaultdict(list)\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('gat')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "1b8a9587fcc1cbb7c2af2866467424141d18584483c14e975ff55eb57b0fa000"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
