
import os
import pandas as pd
import xml.etree.ElementTree as ET

import torch
from torch_geometric.data import InMemoryDataset, Data
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer

from utils.utils import assign_labels, get_edge_list


class MalwareDataset(InMemoryDataset):
    
    def __init__(self, root, cfg, transform=None, pre_transform=None):
        """
        root = where the dataset should be stored
        """
        self.root = root
        self.event_type = cfg['event_type']
        self.transform = transform
        
        super(MalwareDataset, self).__init__(root, transform, pre_transform)
        
    @property
    def raw_file_names(self):
        XML_PATH = os.path.join(self.root, 'xml')
        # if these files do not exists in /raw, the xml parse is not triggered
        return [filename.rsplit('.', 1)[0] + self.event_type + '.csv' for filename in os.listdir(XML_PATH)]
        
    @property
    def processed_file_names(self):
        # if this file exists in /processed, process is not triggered
        return [self.event_type + '.pt'] 
    
    def download(self):
        # process the raw xml files if csv files are not present
        self.process_xml(self.root)

    def len(self):
        # only a single list of graphs is stored
        return 1

    def process_xml(self, base_path):
        print("Processing .XML files...")

        # append raw path to base_dir
        XML_PATH = os.path.join(base_path, 'xml')
        
        # loop through .XML files
        for filename in os.listdir(XML_PATH):
            
            # loop through relevent event type xml files
            if (not filename.endswith(f"{self.event_type}.XML")) and (self.event_type != 'all'):
                continue

            file_key_name = filename.rsplit(".", 1)[0] + '.csv'

            if file_key_name in os.listdir(os.path.join(XML_PATH, os.pardir, 'raw')):
                continue
                
            # get the full path and root of xml file
            path = os.path.join(base_path, 'xml', filename)
            root = ET.parse(path).getroot()
            
            # get the target name of the malware executable
            target_name = filename.split(".")[0].lower()
            
            attr_dict = defaultdict(list)

            # root[0] is process, root[1] is event
            for child in root[0]:
                for event in child:
                    tag, text = event.tag, event.text

                    # get the module list
                    if tag == 'modulelist':

                        # build up the module list
                        empty_stack = []
                        # the path tag is the 4th element in the module list
                        for module in event:
                            empty_stack.append(module[3].text.split('\\')[-1])

                        # create the module feature
                        attr_dict['modules'].append(' '.join(empty_stack))

                    # capture the other keys
                    else:
                        attr_dict[tag].append(text)
                    
            # create save dir and save it
            save_dir = os.path.join(base_path, 'raw')
            os.makedirs(save_dir, exist_ok=True)

            save_name = f'{target_name}.{self.event_type}.csv'
            df = pd.DataFrame(attr_dict).to_csv(
                os.path.join(save_dir, save_name), index=False)
            print(f"Finished saving {save_name} to {save_dir}")

    # main process 
    def process(self):
        
        # parent list for all the executable graphs
        exe_graphs = []
        module_list = []

        # loop through all the files and read csv
        for filename in os.listdir(self.raw_dir):
            
            # check if all is selected, or wrong event type
            if (not filename.endswith(f"{self.event_type}.csv")) and (self.event_type != 'all'):
                continue

            graph_nodes = pd.read_csv(self.raw_dir + '\\' + filename)

            # Assign labels and return array
            target_list = assign_labels(graph_nodes, filename)
            # Get the processed feature list
            module_list.append(graph_nodes['modules'])
            # Get the edge list
            edge_list = get_edge_list(graph_nodes)

            # Create the data object
            data = Data(
                edge_index = edge_list,
                y = target_list
            )
            # append to master list
            exe_graphs.append((filename, data))

        # get the tfidf vectors for the modules
        tfidf_modules =  get_feature_vectors(module_list, ngram=(2, 2), min_df=1)
        
        for graph, module in zip(exe_graphs, tfidf_modules):
            graph[1].x = module 
            graph[1].edge_index = truncate_edges(graph[1].edge_index, graph[1].x.shape[0])
            #graph[1].train_mask, graph[1].val_mask  = random_node_split(graph[1])
        
        # collate the list and save the data
        torch.save([exe[1] for exe in exe_graphs], self.processed_paths[0])

    # fetch the data from the .pt file
    def get(self, idx):
        return torch.load(self.processed_paths[0])


def truncate_edges(edge_index, max_nodes):
    # truncate edges to max_nodes: [2, E]
    edge_index = edge_index[:, edge_index[0] < max_nodes]
    edge_index = edge_index[:, edge_index[1] < max_nodes]
    return edge_index

    # DEPRECATED; wrong random split
def random_node_split(graph, split=0.8):
    # split the data into train, val
    num_nodes = graph.x.shape[0]
    rand_choice = torch.rand((num_nodes,)).le(split)
    return rand_choice, ~rand_choice


# carries out tfidf vectorization on the modules per sample
def get_feature_vectors(module_list, ngram=(2, 2), min_df=1):
    
    new_list = []
    module_shapes = []
    # fill nans with space and collect sizes
    for module in module_list:
        module_shapes.append(module.shape[0])
        new_list.append(module.fillna('', inplace=False).tolist())
    #create flattened list
    flattened_list = [item for sublist in new_list for item in sublist]

    # create the vectorizer and fit
    vectorizer = TfidfVectorizer(ngram_range=ngram,
                                 lowercase=True,
                                 min_df=min_df)
    vectorized_modules = vectorizer.fit_transform(flattened_list).toarray()

    # some processing to make sure the vectors are the correct shape per sample
    vectorized_tfidf = [vectorized_modules[:module_shapes[0]]]
    for start, end in zip(module_shapes[:-1], module_shapes[1:]):
        vectorized_tfidf.append(vectorized_modules[start:start + end])

    return [torch.tensor(exe, dtype=torch.float) for exe in vectorized_tfidf]