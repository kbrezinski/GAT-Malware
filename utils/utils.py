
import torch
import numpy as np
import networkx as nx

from scipy.stats import linregress
from sklearn.datasets import make_blobs
from collections.abc import Iterable

import matplotlib.pyplot as plt

from utils.config import *


def assign_labels(df, file_name):

    """
    Given a file name, this function assigns labels to the processes in the dataframe.
    The label is 1 if the process is malicious, 0 otherwise. The label is assigned
    recursively, starting from the processes that have the same name as the file name.
    """

    prev_len = -1
    target_name = file_name.split(".")[0]

    # get the malicious pids of the parent process
    df['ProcessName'] = df['ProcessName'].apply(lambda x: x.lower())
    unique_pids = df[df['ProcessName'] == target_name + '.exe']['ProcessId'].unique()

    if len(unique_pids) == 0:
        print(f"WARNING: Target name not found in {file_name}")
        prev_len = 0

    mal_pids = set(unique_pids)

    # stops when the length of pids stops growing
    while prev_len != len(mal_pids):
        new_rows = df[df['ParentProcessId'].isin(mal_pids)]
        new_pids = set(new_rows['ProcessId'].unique())

        # updates the lengths and the malicious pids
        prev_len = len(mal_pids)
        mal_pids.update(new_pids)

    df['label'] = df['ProcessId'].apply(lambda x: 1 if x in mal_pids else 0)
    
    return torch.tensor(df['label'].values, dtype=torch.long)


def get_edge_list(graph_nodes, add_self_edges=True):

    """
    Given a dataframe of process nodes, this function returns the edge list
    in the form of a numpy array. The edge list is a 2 x E array, where E is
    the number of edges. The first row contains the source nodes, and the second
    row contains the target nodes.
    """

    ## Create the adjacency list using nx
    G = nx.from_pandas_edgelist(graph_nodes, 'ProcessId', 'ParentProcessId')
    
    ## Create some empty lists
    trg, src = [], []
    pid_lookup = dict()
    
    ## Loop through the adjacency list
    for n, (source, nbrdict) in enumerate(G.adjacency()):

        if source not in pid_lookup:
            pid_lookup[source] = len(pid_lookup)

        for target in nbrdict.keys():
            if target not in pid_lookup:
                pid_lookup[target] = len(pid_lookup)

            src.append(pid_lookup[source])
            trg.append(pid_lookup[target])
            
    ## Add self-edges if set to true
    if add_self_edges:
        src.extend(list(range(len(pid_lookup))))
        trg.extend(list(range(len(pid_lookup))))
        
    ## Convert to numpy arrays [2, E]
    edge_index = np.row_stack((src, trg))
    return torch.tensor(edge_index, dtype=torch.long)


def get_blobs(feature_dim, num_samples=1000,
              split=0.95, cluster_std=[1., 1.],
              transform=None, plot=False):
    
    '''
    Creates some toy datasets that can be used for testing Fractal Dimension approaches.
    Got some ideas from: https://scikit-learn.org/stable/auto_examples/cluster/
    plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py

    '''
    
    # check for low positive training examples
    assert split > .5, f"WARNING: split was set to less than 0.5, did you mean to use {1-split}?"
    
    # check if plot is set to true but wrong feature_dim
    if plot and feature_dim > 2:
        print(f"Feature_dim set to larger than 2 with plot set to false, setting plot to False")
        plot = False
    
    # init proper class sizes and blob centers
    centers = [[-1]*feature_dim, [1]*feature_dim]
    class_sizes = [int(num_samples*split), int((1-split)*num_samples)]
    
    X, y = make_blobs(n_samples=class_sizes, cluster_std=cluster_std, centers=centers)
    
    # carry out anisotropic transformation; only works for feature_dim of 2
    if transform and feature_dim == 2:
        transformation = [[-0.5, 0.5], [-0.5, 1]]
        X = np.dot(X, transformation)
        
    # view plot
    if plot:
        plt.figure(figsize=(5, 4)) 
        plt.scatter(X[:, 0], X[:, 1], alpha=0.6,
                    c=['r' if i == 1 else 'g' for i in y])
        plt.xlabel(r'$X_1$')
        plt.ylabel(r'$X_2$')
        
        #plt.savefig("images/blobs.png", dpi=150, bbox_inches='tight')
        
    return torch.tensor(X, dtype=torch.float32),\
             torch.tensor(y, dtype=torch.long)
        


def mass_fd(X, k=5, skip=0, norm=2, gyration=True, ratios=False, verbose=False):
    '''
    Determine the mass radius or radius of gyration fractal dimension. Numpy implemention used for quick prototyping. 
    
    args:
        X        - (n, m) feature vector. Can be any m-th dimensional data, with n datapoints
        k        - number of scales to use
        skip     - skip some points on the log-log plot. Helps on occasionn for numerical stability in the upper/lower range
        norm     - the norm for the generalized distance metric, where 1 = Manhattan, 2 = Euclidean, 3+ = Minkowski
        gyration - flag to enable radius of gyration instead of mass-radius
        ratios   - flag to enable the use of ratios of std. devs between scales. Doesn't work well :(
        verbose  - plot the log-log plot and print some metadata
        
    returns:
        D        - returns the fractal dimension of the input
        
    
    '''
    # sanity check so enough points are considered for log-log plot
    assert k - skip >= 2, f"Not enough points on the log-log plot to consider, you used: {k=} and {skip=} and need at least 2"
    
    # make X an iterable so that we can run the loop regardless *need to fix this check*
    if not isinstance(X, Iterable) or isinstance(X, np.ndarray):
        X = [X]
    else:
        verbose = False  # remove verbose for Iterable
        
    # main loop
    D = []
    for x in X:
        D.append(mass_fd_aux(x=x, k=k, skip=skip, norm=norm, gyration=gyration, ratios=ratios, verbose=verbose))
        
    # * this is returning an extra dimension for np.array*
    return D


def mass_fd_aux(x, k=5, skip=0, norm=2, gyration=True, ratios=False, verbose=False):
    
    # remove zero values
    x = x[~np.all(x == 0, axis=1)]
    
    # mean along each axis
    centroid = x.mean(axis=0) 

    # find distances and sort by proximity to centroid
    dist = np.power(np.sum(np.abs(x - centroid) ** norm, axis=1), 1. / norm)
    dist_idx_sorted = np.argsort(dist)

    # split distances array into k partitions
    N = np.array_split(dist_idx_sorted, k)
    # points for log-log plot
    Rg = []
    Nk = []

    # loop through scales
    for i in range(k):

        # concatenates sucessive nodes away from centroid (e.g Nk_1, Nk_1 + Nk_2)
        Nj = x[np.concatenate(N[:i+1])]

        # compute the centroid for each k if using ROG
        if gyration:
            centroid = Nj.mean(axis=0)
    
        # compute the std. dev in all d dimensions
        Rg.append(np.power(np.sum(np.abs(Nj - centroid) ** norm) / len(Nj), 1. / norm))
        # append the number of nodes considered
        Nk.append(Nj.shape[0])

    # vanilla implementation, just used the raw std. devs.
    if not ratios:
        log_Rg = np.log(Rg)[:-skip] if skip != 0 else np.log(Rg)
        log_Nk = np.log(Nk)[:-skip] if skip != 0 else np.log(Nk)
        
    # this implementation looks at the ratios of the std. devs. Doesn't work well
    else:
        log_Rg = [np.log(Rg[i] / Rg[i-1]) for i in range(1, len(Rg) - skip)]
        log_Nk = [np.log(Nk[i] / Nk[i-1]) for i in range(1, len(Nk) - skip)]

    # determine the slope as D
    D, *_ = linregress(log_Nk, log_Rg)
    
    # if needed plot the log-log plot and additional metrics
    if verbose:
        print(f"{Rg=}, {Nk=}")
        print(f"{log_Rg=}, {log_Nk=}")
        
        plt.scatter(x[:, 0], x[:, 1])
        plt.show()
        plt.scatter(log_Nk, log_Rg)
        plt.xlabel(f'log(Nk)', fontsize=15)
        plt.ylabel(f'log(Rg)', fontsize=15)

        plt.grid(True)
        plt.tight_layout()
        plt.show()
        
    return D
    
    
