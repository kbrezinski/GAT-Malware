
import os
import pickle
import torch
import numpy as np

from torch import nn
from scipy.stats import linregress
from sklearn.datasets import make_blobs
from collections.abc import Iterable

import matplotlib.pyplot as plt

from utils.config import *


def get_blobs(feature_dim, num_samples=1000,
              split=0.95, cluster_std=[1., 1.],
              transform=None, plot=False):
    
    '''
    Creates some toy datasets that can be used for testing Fractal Dimension approaches. Got some ideas from:
    
        https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html#sphx-glr-auto-examples-cluster-plot-kmeans-assumptions-py

    '''
    
    # check for low positive training examples
    assert split > 0.5, f"WARNING: split was set to less than 0.5, did you mean to use {1-split}?"
    
    # check if plot is set to true but wrong feature_dim
    if plot and feature_dim > 2:
        print(f"Feature_dim set to larger than 2 with plot set to false, setting plot to False")
        plot = False
    
    # init proper class sizes and blob centers
    centers = [[-1]*feature_dim, [1]*feature_dim]
    class_sizes = [int(num_samples*split), int((1-split)*num_samples)]
    
    X, y = make_blobs(n_samples=class_sizes, cluster_std=cluster_std, centers=centers)
    
    # carry out anisotropic transformation; only works for feature_dim of 2
    if transform and feature_dim == 2:
        transformation = [[-0.5, 0.5], [-0.5, 1]]
        X = np.dot(X, transformation)
        
    # view plot
    if plot:
        plt.figure(figsize=(5, 4)) 
        plt.scatter(X[:, 0], X[:, 1], alpha=0.6,
                    c=['r' if i == 1 else 'g' for i in y])
        plt.xlabel(r'$X_1$')
        plt.ylabel(r'$X_2$')
        
        #plt.savefig("images/blobs.png", dpi=150, bbox_inches='tight')
        
    return X, y
        


def import_corpus(event):
    '''
    Import the corpus from the sandy pickled events.
    
    args:
        events  : thread, registry or file
    returns:
        corpus  : list of strings, corresponding to API usage by process
        targets : int value for malicious label, where 1 is malicious 0 is benign
        attr    : dict of attributes, see generate_corpus.py
    
    '''
    file = os.path.join(SANDY_ATTR_PATH, f'corpus.{event}.pkl')
    
    # fetch corpus and attributes
    corpus = pickle.load(open(file, "rb" ))
    attr = pickle.load(open(os.path.join(SANDY_ATTR_PATH, f'attr.{event}.pkl'), "rb" ))
    
    # fetch targets from attribute dict
    targets = attr['target_arr']
    del attr['target_arr']
    
    return corpus, targets, attr




def mass_fd(X, k=5, skip=0, norm=2, gyration=True, ratios=False, verbose=False):
    '''
    Determine the mass radius or radius of gyration fractal dimension. Numpy implemention used for quick
    prototyping. 
    
    args:
        X        - (n, m) feature vector. Can be any m-th dimensional data, with n datapoints
        k        - number of scales to use
        skip     - skip some points on the log-log plot. Helps on occasionn for numerical stability in the upper/lower range
        norm     - the norm for the generalized distance metric, where 1 = Manhattan, 2 = Euclidean, 3+ = Minkowski
        gyration - flag to enable radius of gyration instead of mass-radius
        ratios   - flag to enable the use of ratios of std. devs between scales. Doesn't work well :(
        verbose  - plot the log-log plot and print some metadata
        
    returns:
        D        - returns the fractal dimension of the input
        
    
    '''
    # sanity check so enough points are considered for log-log plot
    assert k - skip >= 2, f"Not enough points on the log-log plot to consider, you used: {k=} and {skip=} and need at least 2"
    
    # make X an iterable so that we can run the loop regardless *need to fix this check*
    if not isinstance(X, Iterable) or isinstance(X, np.ndarray):
        X = [X]
    else:
        verbose = False  # remove verbose for Iterable
        
    # main loop
    D = []
    for x in X:
        D.append(mass_fd_aux(x=x, k=k, skip=skip, norm=norm, gyration=gyration, ratios=ratios, verbose=verbose))
        
    # * this is returning an extra dimension for np.array*
    return D


def mass_fd_aux(x, k=5, skip=0, norm=2, gyration=True, ratios=False, verbose=False):
    
    # remove zero values
    x = x[~np.all(x == 0, axis=1)]
    
    # mean along each axis
    centroid = x.mean(axis=0) 

    # find distances and sort by proximity to centroid
    dist = np.power(np.sum(np.abs(x - centroid) ** norm, axis=1), 1. / norm)
    dist_idx_sorted = np.argsort(dist)

    # split distances array into k partitions
    N = np.array_split(dist_idx_sorted, k)
    # points for log-log plot
    Rg = []
    Nk = []

    # loop through scales
    for i in range(k):

        # concatenates sucessive nodes away from centroid (e.g Nk_1, Nk_1 + Nk_2)
        Nj = x[np.concatenate(N[:i+1])]

        # compute the centroid for each k if using ROG
        if gyration:
            centroid = Nj.mean(axis=0)
    
        # compute the std. dev in all d dimensions
        Rg.append(np.power(np.sum(np.abs(Nj - centroid) ** norm) / len(Nj), 1. / norm))
        # append the number of nodes considered
        Nk.append(Nj.shape[0])

    # vanilla implementation, just used the raw std. devs.
    if not ratios:
        log_Rg = np.log(Rg)[:-skip] if skip != 0 else np.log(Rg)
        log_Nk = np.log(Nk)[:-skip] if skip != 0 else np.log(Nk)
        
    # this implementation looks at the ratios of the std. devs. Doesn't work well
    else:
        log_Rg = [np.log(Rg[i] / Rg[i-1]) for i in range(1, len(Rg) - skip)]
        log_Nk = [np.log(Nk[i] / Nk[i-1]) for i in range(1, len(Nk) - skip)]

    # determine the slope as D
    D, *_ = linregress(log_Nk, log_Rg)
    
    # if needed plot the log-log plot and additional metrics
    if verbose:
        print(f"{Rg=}, {Nk=}")
        print(f"{log_Rg=}, {log_Nk=}")
        
        plt.scatter(x[:, 0], x[:, 1])
        plt.show()
        plt.scatter(log_Nk, log_Rg)
        plt.xlabel(f'log(Nk)', fontsize=15)
        plt.ylabel(f'log(Rg)', fontsize=15)

        plt.grid(True)
        plt.tight_layout()
        plt.show()
        
    return D
    
    


class Mass_fd(nn.Module):
    '''
    Determine the mass radius or radius of gyration fractal dimension for a torch implementation. Same params as the vanilla
    implementation, but removed some experimental settings
    
    args:
        x      - (n, m) feature vector. Can be any m-th dimensional data, with n datapoints
        k        - number of scales to use
        skip     - skip some points on the log-log plot. Helps on occasionn for numerical stability in the upper/lower range
        gyration - flag to enable radius of gyration instead of mass-radius
        verbose  - plot the log-log plot and print some metadata
        
    returns:
        D        - returns the fractal dimension of the input
    '''
    
    def __init__(self, out_dim, k=5, skip=0, gyration=True, weight=False, norm=2.):
        super().__init__()
        
        assert k - skip >= 2, f"Not enough points on the log-log plot to consider, you used: {k=} and {skip=} but need at least 2"
        
        self.out_dim = out_dim
        
        self.k = k
        self.skip = skip
        self.gyration = gyration

        # create the trainable norm parameter
        if norm is None:
            self.norm = nn.Parameter(torch.Tensor([2.]))
        else:
            self.norm = norm
                 
        # create the bias layer
        if weight:
            self.weight = nn.Parameter(torch.ones((out_dim,)))
        else:
            self.register_parameter('weight', None)
        
    def extra_repr(self):
        return 'out_dim={}, k={}, skip={}, gyration={}'.format(
            self.out_dim, self.k, self.skip, self.gyration
        )
        
    def forward(self, x):
        
        # remove zero values; useful for numeric stability when n_dim is low
        x = x[~torch.all(x == 0., axis=1)]

        # mean along each axis
        centroid = x.mean(axis=0) 

        # find distances and sort by proximity to centroid       
        dist = torch.pow(torch.sum(torch.abs(x - centroid) ** self.norm, axis=1), 1. / self.norm)
        dist_idx_sorted = torch.argsort(dist)

        # split distances array into k partitions
        N = np.array_split(dist_idx_sorted, self.k)
        # points for log-log plot
        Rg = []
        Nk = []

        # loop through scales
        for i in range(self.k):

            # concatenates sucessive nodes away from centroid (e.g Nk_1, Nk_1 + Nk_2)
            Nj = x[torch.cat(N[:i+1])]

            # compute the centroid for each k if using ROG
            if self.gyration:
                centroid = torch.mean(Nj, axis=0)

            # compute the std. dev in all d dimensions; can use mean instead of sum
            Rg.append(torch.pow(torch.sum(torch.abs(Nj - centroid) ** self.norm) / len(Nj), 1. / self.norm))
            # append the number of nodes considered
            Nk.append(Nj.shape[0])

        # convert lists to tensors to make them (k, 1) matrices
        Rg = torch.Tensor(Rg).unsqueeze(-1).requires_grad_()
        Nk = torch.Tensor(Nk).unsqueeze(-1).requires_grad_()

        # vanilla implementation, just used the raw std. devs.
        log_Rg = torch.log(Rg)[:-self.skip] if self.skip != 0 else torch.log(Rg)
        log_Nk = torch.log(Nk)[:-self.skip] if self.skip != 0 else torch.log(Nk)

        # closed-form least squares linear regression (X.T X)^-1 X.T y
        D = torch.matmul(torch.matmul(
            torch.cholesky_inverse(torch.matmul(log_Rg.T, log_Rg)), log_Rg.T), log_Nk)
    
        # CHECK THIS
        return torch.mul(D, self.weight) if self.weight else D